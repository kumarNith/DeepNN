{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.5.0\n"
     ]
    }
   ],
   "source": [
    "print (\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = mnist_data.read_data_sets(\"data\", one_hot=True)#, reshape=True, validation_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createPlaceHolders(n_width, n_height, n_depth, n_classes):\n",
    "    X = tf.placeholder(tf.float32, [None,n_width, n_height, n_depth ])\n",
    "    y = tf.placeholder(tf.float32,  [None, n_classes])\n",
    "    step = tf.placeholder(tf.int32)\n",
    "    return X,y,step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = tf.reshape(X_, shape = [-1, n_width, n_height, n_depth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W1 = tf.get_variable(\"W1\", shape=[5,5,1,K], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1 = tf.Variable(tf.ones([K])/10)\n",
    "W2 = tf.get_variable(\"W2\", shape=[5,5,K,L], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2 = tf.Variable(tf.ones([L])/10)\n",
    "W3 = tf.get_variable(\"W3\", shape=[4,4,L,M], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B3 = tf.Variable(tf.ones([M])/10)\n",
    "W4 = tf.get_variable(\"W4\", shape=[7,7,M,N], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B4 = tf.Variable(tf.ones([N])/10)\n",
    "W5 = tf.get_variable(\"W5\", shape=[N,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B5 = tf.Variable(tf.ones([class_size])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intialize_parameters(K,L,M,N):\n",
    "    parameters = {}\n",
    "    W1 = tf.get_variable('W1', shape = [5, 5, 1, K], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    #tf.Variable(tf.truncated_normal([5, 5, 1, K], stddev=0.1))  # 5x5 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.ones([K])/10)\n",
    "    W2 = tf.get_variable('W2', shape=[5, 5, K, L], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    #tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "    B2 = tf.Variable(tf.ones([L])/10)\n",
    "    W3 = tf.get_variable('W3', shape=[4, 4, L, M], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    #tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "    B3 = tf.Variable(tf.ones([M])/10)\n",
    "    W4 = tf.get_variable('W4', shape=[7 * 7 * M, N], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    B4 = tf.Variable(tf.ones([N])/10)\n",
    "    W5 = tf.get_variable('W5', shape=[N, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    B5 = tf.Variable(tf.ones([10])/10)\n",
    "    parameters = {\"W1\":W1, \"B1\":B1, \"W2\":W2, \"B2\":B2,\"W3\":W3,\"B3\":B3,\"W4\":W4, \"B4\":B4,\"W5\":W5, \"B5\":B5}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(parameters,X, M, n_inputs, n_classes):\n",
    "    #X,y = createPlaceHolders(n_inputs, n_classes)\n",
    "    #parameters = intialize_parameter(K,L,M,N)\n",
    "    s = 1 # stride\n",
    "    conv1 = tf.nn.relu(tf.nn.conv2d(X, parameters['W1'], strides=[1,s,s,1], padding = 'SAME') + parameters['B1'])\n",
    "    \n",
    "    #max pooling\n",
    "    pool1 = tf.nn.max_pool()\n",
    "\n",
    "    s = 2 # stride\n",
    "    y2 = tf.nn.relu(tf.nn.conv2d(y1, parameters['W2'], strides=[1,s,s,1], padding = 'SAME') + parameters['B2'])\n",
    "\n",
    "    s = 2 # stride\n",
    "    y3 = tf.nn.relu(tf.nn.conv2d(y2, parameters['W3'], strides=[1,s,s,1], padding = 'SAME') + parameters['B3'])\n",
    "\n",
    "    #falttern\n",
    "    yy = tf.reshape(y3,  [-1, 7 * 7 * M])  # shape =\n",
    "    y4 = tf.nn.relu(tf.matmul(yy, parameters['W4']) + parameters['B4'])\n",
    "    yLogit = tf.matmul(y4, parameters['W5']) + parameters['B5']\n",
    "\n",
    "    \n",
    "    \n",
    "    return yLogit\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_loss(yLogit, y):\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits=yLogit, labels=y)\n",
    "    loss = tf.reduce_mean(entropy)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_accuracy(yLogit, y):\n",
    "    prediction = tf.nn.softmax(yLogit)\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    mnist = mnist_data.read_data_sets(\"data\", one_hot=True)\n",
    "    \n",
    "    n_classes = 10\n",
    "    n_width   = 28\n",
    "    n_height  = 28\n",
    "    n_depth   = 1\n",
    "    n_inputs = n_height * n_width * n_depth # total pixels\n",
    "    \n",
    "    # three convolution layers \n",
    "    K = 24   # First layer depth\n",
    "    L = 48   # Second layer depth\n",
    "    M = 64  # Third layer depth\n",
    "    N = 300 # Fully connected \n",
    "\n",
    "    learning_rate = 0.001\n",
    "    n_epochs = 10\n",
    "    epochs = 550\n",
    "    batch_size = 100\n",
    "    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    \n",
    "    X, y, step = createPlaceHolders(n_width, n_height, n_depth, n_classes)\n",
    "    parameters = intialize_parameters(K, L, M, N)\n",
    "    yLogit     = predict(parameters, X, M, n_inputs, n_classes)\n",
    "    loss       = find_loss(yLogit, y)\n",
    "    accuracy   = find_accuracy(yLogit, y)\n",
    "    \n",
    "    #lr = 0.0001 +  tf.train.exponential_decay(0.003, step, 2000, 1/math.e)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    \n",
    "    acc = 0.0\n",
    "    lo = 0.0\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    for i in xrange(epochs):\n",
    "        batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "        a, c, _ = sess.run([accuracy, loss , optimizer], feed_dict={X : batch_X.reshape([-1, n_width, n_height, n_depth]), y : batch_y, step : i})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c))\n",
    "        acc = a + acc\n",
    "        lo = c + lo\n",
    "    print acc,lo\n",
    "    print('Train accuracy is  ', acc/epochs)\n",
    "    print('Train loss is  ', lo/epochs)\n",
    "    acc = 0.0\n",
    "    lo = 0.0\n",
    "    for i in xrange(100):\n",
    "        batch_X, batch_y = mnist.test.next_batch(batch_size)\n",
    "        a, c = sess.run([accuracy, loss],\n",
    "                                    feed_dict={X : batch_X.reshape([-1, n_width, n_height, n_depth]) , y : batch_y })\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        acc = a + acc\n",
    "        lo = c+lo\n",
    "    print acc, lo\n",
    "    print('Test accuracy is  ', acc/100)\n",
    "    print('Test loss is  ', lo/100)\n",
    "    sess.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "0: accuracy:0.08 loss: 2.3071601\n",
      "1: accuracy:0.13 loss: 2.5428808\n",
      "2: accuracy:0.22 loss: 2.4272718\n",
      "3: accuracy:0.21 loss: 2.355474\n",
      "4: accuracy:0.14 loss: 2.243554\n",
      "5: accuracy:0.45 loss: 2.2057447\n",
      "6: accuracy:0.28 loss: 2.1616373\n",
      "7: accuracy:0.22 loss: 2.2180433\n",
      "8: accuracy:0.38 loss: 2.1213229\n",
      "9: accuracy:0.61 loss: 1.99426\n",
      "10: accuracy:0.51 loss: 1.9238063\n",
      "11: accuracy:0.49 loss: 1.8067737\n",
      "12: accuracy:0.49 loss: 1.741659\n",
      "13: accuracy:0.75 loss: 1.4177612\n",
      "14: accuracy:0.69 loss: 1.3690314\n",
      "15: accuracy:0.73 loss: 1.143476\n",
      "16: accuracy:0.72 loss: 0.9814473\n",
      "17: accuracy:0.74 loss: 0.8174007\n",
      "18: accuracy:0.77 loss: 0.857941\n",
      "19: accuracy:0.76 loss: 0.7116574\n",
      "20: accuracy:0.84 loss: 0.60502684\n",
      "21: accuracy:0.76 loss: 0.5945478\n",
      "22: accuracy:0.83 loss: 0.6836641\n",
      "23: accuracy:0.8 loss: 0.59996134\n",
      "24: accuracy:0.87 loss: 0.45117703\n",
      "25: accuracy:0.76 loss: 0.84023607\n",
      "26: accuracy:0.88 loss: 0.47415313\n",
      "27: accuracy:0.87 loss: 0.5079485\n",
      "28: accuracy:0.82 loss: 0.5257405\n",
      "29: accuracy:0.87 loss: 0.42332414\n",
      "30: accuracy:0.85 loss: 0.45117772\n",
      "31: accuracy:0.88 loss: 0.46641156\n",
      "32: accuracy:0.85 loss: 0.41805977\n",
      "33: accuracy:0.85 loss: 0.61008537\n",
      "34: accuracy:0.85 loss: 0.6001033\n",
      "35: accuracy:0.83 loss: 0.51263094\n",
      "36: accuracy:0.91 loss: 0.34350875\n",
      "37: accuracy:0.92 loss: 0.31210527\n",
      "38: accuracy:0.89 loss: 0.4086672\n",
      "39: accuracy:0.78 loss: 0.6671141\n",
      "40: accuracy:0.85 loss: 0.509071\n",
      "41: accuracy:0.91 loss: 0.35691714\n",
      "42: accuracy:0.91 loss: 0.26064822\n",
      "43: accuracy:0.88 loss: 0.36473542\n",
      "44: accuracy:0.85 loss: 0.5290062\n",
      "45: accuracy:0.91 loss: 0.48222512\n",
      "46: accuracy:0.86 loss: 0.35948074\n",
      "47: accuracy:0.87 loss: 0.4599623\n",
      "48: accuracy:0.92 loss: 0.31524926\n",
      "49: accuracy:0.9 loss: 0.40165862\n",
      "50: accuracy:0.86 loss: 0.38320243\n",
      "51: accuracy:0.91 loss: 0.27289224\n",
      "52: accuracy:0.93 loss: 0.27162743\n",
      "53: accuracy:0.92 loss: 0.34354103\n",
      "54: accuracy:0.91 loss: 0.2557061\n",
      "55: accuracy:0.95 loss: 0.24828006\n",
      "56: accuracy:0.91 loss: 0.2782073\n",
      "57: accuracy:0.84 loss: 0.41693124\n",
      "58: accuracy:0.92 loss: 0.3215803\n",
      "59: accuracy:0.92 loss: 0.2634986\n",
      "60: accuracy:0.85 loss: 0.41356415\n",
      "61: accuracy:0.95 loss: 0.24266642\n",
      "62: accuracy:0.94 loss: 0.21439293\n",
      "63: accuracy:0.9 loss: 0.26588684\n",
      "64: accuracy:0.94 loss: 0.1997297\n",
      "65: accuracy:0.97 loss: 0.19267356\n",
      "66: accuracy:0.89 loss: 0.4335809\n",
      "67: accuracy:0.94 loss: 0.2732728\n",
      "68: accuracy:0.89 loss: 0.29233325\n",
      "69: accuracy:0.94 loss: 0.23769556\n",
      "70: accuracy:0.95 loss: 0.22466339\n",
      "71: accuracy:0.9 loss: 0.29563355\n",
      "72: accuracy:0.94 loss: 0.21280003\n",
      "73: accuracy:0.95 loss: 0.28288376\n",
      "74: accuracy:0.94 loss: 0.18585995\n",
      "75: accuracy:0.88 loss: 0.36637902\n",
      "76: accuracy:0.9 loss: 0.3247143\n",
      "77: accuracy:0.94 loss: 0.28175044\n",
      "78: accuracy:0.93 loss: 0.35257232\n",
      "79: accuracy:0.95 loss: 0.20640564\n",
      "80: accuracy:0.9 loss: 0.26265132\n",
      "81: accuracy:0.96 loss: 0.155802\n",
      "82: accuracy:0.93 loss: 0.17973159\n",
      "83: accuracy:0.92 loss: 0.26006463\n",
      "84: accuracy:0.96 loss: 0.2067979\n",
      "85: accuracy:0.91 loss: 0.22921972\n",
      "86: accuracy:0.91 loss: 0.47672057\n",
      "87: accuracy:0.93 loss: 0.24288027\n",
      "88: accuracy:0.95 loss: 0.18623269\n",
      "89: accuracy:0.95 loss: 0.13600263\n",
      "90: accuracy:0.97 loss: 0.14390746\n",
      "91: accuracy:0.94 loss: 0.16284557\n",
      "92: accuracy:0.97 loss: 0.16731809\n",
      "93: accuracy:0.96 loss: 0.22065315\n",
      "94: accuracy:0.95 loss: 0.21641901\n",
      "95: accuracy:0.94 loss: 0.21975233\n",
      "96: accuracy:0.94 loss: 0.19158751\n",
      "97: accuracy:0.9 loss: 0.28516483\n",
      "98: accuracy:0.93 loss: 0.32108882\n",
      "99: accuracy:0.95 loss: 0.22762546\n",
      "100: accuracy:0.97 loss: 0.1607402\n",
      "101: accuracy:0.93 loss: 0.18999954\n",
      "102: accuracy:0.95 loss: 0.24492653\n",
      "103: accuracy:0.89 loss: 0.24932598\n",
      "104: accuracy:0.95 loss: 0.23274784\n",
      "105: accuracy:0.92 loss: 0.23566657\n",
      "106: accuracy:0.91 loss: 0.27672836\n",
      "107: accuracy:0.91 loss: 0.2581271\n",
      "108: accuracy:0.92 loss: 0.20183407\n",
      "109: accuracy:0.92 loss: 0.2080991\n",
      "110: accuracy:0.96 loss: 0.1287984\n",
      "111: accuracy:0.91 loss: 0.21237282\n",
      "112: accuracy:0.95 loss: 0.17996196\n",
      "113: accuracy:0.96 loss: 0.15721735\n",
      "114: accuracy:0.98 loss: 0.069854945\n",
      "115: accuracy:0.94 loss: 0.30587336\n",
      "116: accuracy:0.95 loss: 0.19234495\n",
      "117: accuracy:0.94 loss: 0.25275823\n",
      "118: accuracy:0.94 loss: 0.15769094\n",
      "119: accuracy:0.96 loss: 0.12693861\n",
      "120: accuracy:0.96 loss: 0.17602499\n",
      "121: accuracy:0.93 loss: 0.25421533\n",
      "122: accuracy:0.94 loss: 0.22634567\n",
      "123: accuracy:0.95 loss: 0.15066823\n",
      "124: accuracy:0.95 loss: 0.13885263\n",
      "125: accuracy:0.96 loss: 0.21709442\n",
      "126: accuracy:0.92 loss: 0.23212467\n",
      "127: accuracy:0.9 loss: 0.30562603\n",
      "128: accuracy:0.91 loss: 0.29371125\n",
      "129: accuracy:0.94 loss: 0.14783055\n",
      "130: accuracy:0.92 loss: 0.2649652\n",
      "131: accuracy:0.98 loss: 0.12007837\n",
      "132: accuracy:0.96 loss: 0.1360338\n",
      "133: accuracy:0.94 loss: 0.14147529\n",
      "134: accuracy:0.92 loss: 0.21680571\n",
      "135: accuracy:0.94 loss: 0.2044619\n",
      "136: accuracy:0.99 loss: 0.04593486\n",
      "137: accuracy:0.92 loss: 0.21951412\n",
      "138: accuracy:0.97 loss: 0.15187277\n",
      "139: accuracy:0.96 loss: 0.09938267\n",
      "140: accuracy:0.98 loss: 0.07618057\n",
      "141: accuracy:0.94 loss: 0.18301488\n",
      "142: accuracy:0.92 loss: 0.25632337\n",
      "143: accuracy:0.96 loss: 0.09232142\n",
      "144: accuracy:0.97 loss: 0.10472124\n",
      "145: accuracy:0.97 loss: 0.10651806\n",
      "146: accuracy:0.97 loss: 0.1355996\n",
      "147: accuracy:0.98 loss: 0.08751396\n",
      "148: accuracy:0.97 loss: 0.103940636\n",
      "149: accuracy:0.95 loss: 0.15433714\n",
      "150: accuracy:0.94 loss: 0.20231746\n",
      "151: accuracy:0.97 loss: 0.089658625\n",
      "152: accuracy:0.95 loss: 0.11865868\n",
      "153: accuracy:0.95 loss: 0.11541588\n",
      "154: accuracy:0.96 loss: 0.15583007\n",
      "155: accuracy:0.92 loss: 0.21801467\n",
      "156: accuracy:0.97 loss: 0.11459507\n",
      "157: accuracy:0.96 loss: 0.15528674\n",
      "158: accuracy:0.96 loss: 0.1279571\n",
      "159: accuracy:0.97 loss: 0.15064357\n",
      "160: accuracy:0.95 loss: 0.13395447\n",
      "161: accuracy:0.98 loss: 0.088959806\n",
      "162: accuracy:0.98 loss: 0.08609166\n",
      "163: accuracy:0.96 loss: 0.13064039\n",
      "164: accuracy:0.96 loss: 0.10890627\n",
      "165: accuracy:0.98 loss: 0.10121676\n",
      "166: accuracy:0.97 loss: 0.11101889\n",
      "167: accuracy:0.96 loss: 0.13017274\n",
      "168: accuracy:0.99 loss: 0.11160798\n",
      "169: accuracy:0.93 loss: 0.1605867\n",
      "170: accuracy:0.96 loss: 0.106933385\n",
      "171: accuracy:0.99 loss: 0.052445304\n",
      "172: accuracy:0.96 loss: 0.16061707\n",
      "173: accuracy:0.96 loss: 0.093158014\n",
      "174: accuracy:0.93 loss: 0.26021346\n",
      "175: accuracy:0.98 loss: 0.061201192\n",
      "176: accuracy:0.95 loss: 0.1581669\n",
      "177: accuracy:0.97 loss: 0.16333298\n",
      "178: accuracy:0.98 loss: 0.05482124\n",
      "179: accuracy:0.97 loss: 0.12417934\n",
      "180: accuracy:0.97 loss: 0.14990768\n",
      "181: accuracy:0.99 loss: 0.060157266\n",
      "182: accuracy:0.95 loss: 0.13743149\n",
      "183: accuracy:0.98 loss: 0.068881154\n",
      "184: accuracy:1.0 loss: 0.023689661\n",
      "185: accuracy:0.96 loss: 0.13875955\n",
      "186: accuracy:0.92 loss: 0.21412438\n",
      "187: accuracy:0.96 loss: 0.10131588\n",
      "188: accuracy:0.98 loss: 0.0740003\n",
      "189: accuracy:0.92 loss: 0.26120603\n",
      "190: accuracy:0.97 loss: 0.0942336\n",
      "191: accuracy:0.96 loss: 0.09495416\n",
      "192: accuracy:0.96 loss: 0.088694185\n",
      "193: accuracy:0.98 loss: 0.08462593\n",
      "194: accuracy:0.96 loss: 0.11388478\n",
      "195: accuracy:0.96 loss: 0.12721847\n",
      "196: accuracy:1.0 loss: 0.056930017\n",
      "197: accuracy:0.97 loss: 0.10237326\n",
      "198: accuracy:0.98 loss: 0.14421976\n",
      "199: accuracy:0.98 loss: 0.07845418\n",
      "200: accuracy:0.96 loss: 0.16528976\n",
      "201: accuracy:0.96 loss: 0.13156441\n",
      "202: accuracy:0.98 loss: 0.08877573\n",
      "203: accuracy:0.97 loss: 0.1278235\n",
      "204: accuracy:0.98 loss: 0.08154354\n",
      "205: accuracy:0.97 loss: 0.11016347\n",
      "206: accuracy:0.95 loss: 0.15605538\n",
      "207: accuracy:0.93 loss: 0.22044876\n",
      "208: accuracy:0.98 loss: 0.057777595\n",
      "209: accuracy:1.0 loss: 0.030337673\n",
      "210: accuracy:0.99 loss: 0.044866018\n",
      "211: accuracy:0.99 loss: 0.05276356\n",
      "212: accuracy:0.95 loss: 0.19430363\n",
      "213: accuracy:0.97 loss: 0.11632282\n",
      "214: accuracy:0.98 loss: 0.061984543\n",
      "215: accuracy:0.97 loss: 0.09651613\n",
      "216: accuracy:0.97 loss: 0.10432632\n",
      "217: accuracy:0.96 loss: 0.11041084\n",
      "218: accuracy:0.95 loss: 0.19334781\n",
      "219: accuracy:0.99 loss: 0.06634255\n",
      "220: accuracy:0.98 loss: 0.05209073\n",
      "221: accuracy:0.98 loss: 0.040500924\n",
      "222: accuracy:0.96 loss: 0.10656742\n",
      "223: accuracy:0.96 loss: 0.139117\n",
      "224: accuracy:0.96 loss: 0.11988903\n",
      "225: accuracy:0.97 loss: 0.12895064\n",
      "226: accuracy:1.0 loss: 0.024926867\n",
      "227: accuracy:1.0 loss: 0.026009714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228: accuracy:0.98 loss: 0.074018605\n",
      "229: accuracy:0.93 loss: 0.23693833\n",
      "230: accuracy:0.98 loss: 0.06617777\n",
      "231: accuracy:0.96 loss: 0.18262435\n",
      "232: accuracy:0.98 loss: 0.091337465\n",
      "233: accuracy:0.97 loss: 0.07337758\n",
      "234: accuracy:0.95 loss: 0.08443007\n",
      "235: accuracy:0.97 loss: 0.16892937\n",
      "236: accuracy:0.98 loss: 0.050849203\n",
      "237: accuracy:0.97 loss: 0.090956725\n",
      "238: accuracy:0.98 loss: 0.058062825\n",
      "239: accuracy:0.94 loss: 0.2039483\n",
      "240: accuracy:0.94 loss: 0.16756457\n",
      "241: accuracy:0.98 loss: 0.077507354\n",
      "242: accuracy:1.0 loss: 0.032516614\n",
      "243: accuracy:0.95 loss: 0.105700165\n",
      "244: accuracy:0.97 loss: 0.09734952\n",
      "245: accuracy:0.98 loss: 0.068531305\n",
      "246: accuracy:0.97 loss: 0.10094462\n",
      "247: accuracy:0.99 loss: 0.035178937\n",
      "248: accuracy:0.98 loss: 0.049404144\n",
      "249: accuracy:0.98 loss: 0.07666638\n",
      "250: accuracy:0.98 loss: 0.054590885\n",
      "251: accuracy:0.96 loss: 0.11755136\n",
      "252: accuracy:0.97 loss: 0.08513336\n",
      "253: accuracy:0.98 loss: 0.07039977\n",
      "254: accuracy:0.95 loss: 0.16076817\n",
      "255: accuracy:0.98 loss: 0.050809003\n",
      "256: accuracy:0.97 loss: 0.08996602\n",
      "257: accuracy:0.9 loss: 0.2956469\n",
      "258: accuracy:0.97 loss: 0.06937268\n",
      "259: accuracy:0.97 loss: 0.10278292\n",
      "260: accuracy:0.99 loss: 0.07860114\n",
      "261: accuracy:0.97 loss: 0.1242985\n",
      "262: accuracy:0.95 loss: 0.19287476\n",
      "263: accuracy:0.98 loss: 0.04889306\n",
      "264: accuracy:0.96 loss: 0.1363172\n",
      "265: accuracy:0.95 loss: 0.11606774\n",
      "266: accuracy:0.97 loss: 0.18951641\n",
      "267: accuracy:1.0 loss: 0.020362353\n",
      "268: accuracy:0.97 loss: 0.14751488\n",
      "269: accuracy:0.99 loss: 0.05467008\n",
      "270: accuracy:0.94 loss: 0.12899768\n",
      "271: accuracy:1.0 loss: 0.043066464\n",
      "272: accuracy:0.97 loss: 0.11340559\n",
      "273: accuracy:0.99 loss: 0.049113754\n",
      "274: accuracy:0.98 loss: 0.14748092\n",
      "275: accuracy:0.96 loss: 0.16937745\n",
      "276: accuracy:0.98 loss: 0.056747492\n",
      "277: accuracy:0.97 loss: 0.063762605\n",
      "278: accuracy:0.96 loss: 0.068664275\n",
      "279: accuracy:0.99 loss: 0.06683288\n",
      "280: accuracy:0.97 loss: 0.060250964\n",
      "281: accuracy:0.97 loss: 0.07423503\n",
      "282: accuracy:0.98 loss: 0.07215054\n",
      "283: accuracy:0.96 loss: 0.16631359\n",
      "284: accuracy:0.98 loss: 0.045915045\n",
      "285: accuracy:0.99 loss: 0.039493077\n",
      "286: accuracy:0.96 loss: 0.1104005\n",
      "287: accuracy:0.99 loss: 0.031612206\n",
      "288: accuracy:0.96 loss: 0.08384787\n",
      "289: accuracy:0.99 loss: 0.0548733\n",
      "290: accuracy:0.98 loss: 0.091009505\n",
      "291: accuracy:0.97 loss: 0.13262214\n",
      "292: accuracy:0.98 loss: 0.12043038\n",
      "293: accuracy:0.97 loss: 0.11183153\n",
      "294: accuracy:0.99 loss: 0.06630117\n",
      "295: accuracy:0.97 loss: 0.06497198\n",
      "296: accuracy:0.99 loss: 0.02909552\n",
      "297: accuracy:0.98 loss: 0.045470357\n",
      "298: accuracy:0.98 loss: 0.094603956\n",
      "299: accuracy:1.0 loss: 0.021543322\n",
      "300: accuracy:0.99 loss: 0.036980465\n",
      "301: accuracy:0.99 loss: 0.056752175\n",
      "302: accuracy:0.92 loss: 0.3325608\n",
      "303: accuracy:0.93 loss: 0.2482513\n",
      "304: accuracy:1.0 loss: 0.0048058634\n",
      "305: accuracy:0.97 loss: 0.07458141\n",
      "306: accuracy:0.96 loss: 0.08287726\n",
      "307: accuracy:0.97 loss: 0.08426149\n",
      "308: accuracy:0.99 loss: 0.032406334\n",
      "309: accuracy:0.98 loss: 0.0685758\n",
      "310: accuracy:0.98 loss: 0.08172754\n",
      "311: accuracy:0.97 loss: 0.10843096\n",
      "312: accuracy:0.97 loss: 0.09774022\n",
      "313: accuracy:0.98 loss: 0.0661871\n",
      "314: accuracy:0.93 loss: 0.18549633\n",
      "315: accuracy:0.95 loss: 0.13455114\n",
      "316: accuracy:0.97 loss: 0.08361963\n",
      "317: accuracy:0.97 loss: 0.09447473\n",
      "318: accuracy:0.98 loss: 0.0794651\n",
      "319: accuracy:0.98 loss: 0.119862564\n",
      "320: accuracy:1.0 loss: 0.018539907\n",
      "321: accuracy:0.95 loss: 0.10922489\n",
      "322: accuracy:0.93 loss: 0.21689862\n",
      "323: accuracy:0.99 loss: 0.045627307\n",
      "324: accuracy:0.94 loss: 0.13629454\n",
      "325: accuracy:0.97 loss: 0.07070518\n",
      "326: accuracy:0.99 loss: 0.048862033\n",
      "327: accuracy:0.96 loss: 0.073654026\n",
      "328: accuracy:0.98 loss: 0.037615295\n",
      "329: accuracy:0.98 loss: 0.07080157\n",
      "330: accuracy:0.97 loss: 0.1727664\n",
      "331: accuracy:0.99 loss: 0.059410118\n",
      "332: accuracy:0.96 loss: 0.10814848\n",
      "333: accuracy:0.96 loss: 0.086628\n",
      "334: accuracy:0.96 loss: 0.1964778\n",
      "335: accuracy:0.99 loss: 0.045397952\n",
      "336: accuracy:0.96 loss: 0.09995465\n",
      "337: accuracy:0.97 loss: 0.082364276\n",
      "338: accuracy:0.99 loss: 0.051870808\n",
      "339: accuracy:0.99 loss: 0.048602752\n",
      "340: accuracy:0.95 loss: 0.12134838\n",
      "341: accuracy:0.98 loss: 0.050919704\n",
      "342: accuracy:0.97 loss: 0.06936235\n",
      "343: accuracy:0.97 loss: 0.14824226\n",
      "344: accuracy:0.99 loss: 0.059377044\n",
      "345: accuracy:0.97 loss: 0.062394742\n",
      "346: accuracy:0.99 loss: 0.066083364\n",
      "347: accuracy:0.97 loss: 0.09068227\n",
      "348: accuracy:0.95 loss: 0.24335833\n",
      "349: accuracy:0.96 loss: 0.07905549\n",
      "350: accuracy:0.94 loss: 0.1856001\n",
      "351: accuracy:0.99 loss: 0.046064973\n",
      "352: accuracy:0.99 loss: 0.032594\n",
      "353: accuracy:0.98 loss: 0.044346027\n",
      "354: accuracy:0.96 loss: 0.07479343\n",
      "355: accuracy:0.99 loss: 0.039919093\n",
      "356: accuracy:0.94 loss: 0.13270128\n",
      "357: accuracy:0.94 loss: 0.18074124\n",
      "358: accuracy:0.98 loss: 0.069485374\n",
      "359: accuracy:0.99 loss: 0.051802192\n",
      "360: accuracy:0.97 loss: 0.111244924\n",
      "361: accuracy:0.99 loss: 0.06622795\n",
      "362: accuracy:0.99 loss: 0.053971548\n",
      "363: accuracy:0.99 loss: 0.03299506\n",
      "364: accuracy:0.99 loss: 0.04629782\n",
      "365: accuracy:0.93 loss: 0.25277892\n",
      "366: accuracy:0.99 loss: 0.07486472\n",
      "367: accuracy:0.95 loss: 0.09829393\n",
      "368: accuracy:0.99 loss: 0.08370792\n",
      "369: accuracy:0.98 loss: 0.09667694\n",
      "370: accuracy:0.99 loss: 0.04252158\n",
      "371: accuracy:0.99 loss: 0.033424206\n",
      "372: accuracy:0.97 loss: 0.091510735\n",
      "373: accuracy:0.99 loss: 0.05437953\n",
      "374: accuracy:0.99 loss: 0.03434133\n",
      "375: accuracy:0.99 loss: 0.04114396\n",
      "376: accuracy:0.97 loss: 0.0766961\n",
      "377: accuracy:0.95 loss: 0.10027256\n",
      "378: accuracy:0.98 loss: 0.054598063\n",
      "379: accuracy:0.98 loss: 0.060714647\n",
      "380: accuracy:0.96 loss: 0.07081155\n",
      "381: accuracy:0.98 loss: 0.062819384\n",
      "382: accuracy:0.97 loss: 0.07863492\n",
      "383: accuracy:0.99 loss: 0.0229148\n",
      "384: accuracy:1.0 loss: 0.016645644\n",
      "385: accuracy:0.97 loss: 0.08848634\n",
      "386: accuracy:0.99 loss: 0.02999545\n",
      "387: accuracy:0.98 loss: 0.051495217\n",
      "388: accuracy:0.99 loss: 0.07146494\n",
      "389: accuracy:0.97 loss: 0.07403327\n",
      "390: accuracy:1.0 loss: 0.025413113\n",
      "391: accuracy:0.98 loss: 0.06965237\n",
      "392: accuracy:0.99 loss: 0.07812798\n",
      "393: accuracy:0.99 loss: 0.04777489\n",
      "394: accuracy:0.96 loss: 0.11021478\n",
      "395: accuracy:0.97 loss: 0.08496349\n",
      "396: accuracy:0.98 loss: 0.06302654\n",
      "397: accuracy:0.99 loss: 0.04480983\n",
      "398: accuracy:1.0 loss: 0.02439625\n",
      "399: accuracy:0.99 loss: 0.06283282\n",
      "400: accuracy:0.99 loss: 0.035684563\n",
      "401: accuracy:0.97 loss: 0.076862074\n",
      "402: accuracy:0.97 loss: 0.08974843\n",
      "403: accuracy:0.97 loss: 0.12750168\n",
      "404: accuracy:0.98 loss: 0.08005898\n",
      "405: accuracy:1.0 loss: 0.016618883\n",
      "406: accuracy:0.98 loss: 0.0808414\n",
      "407: accuracy:0.98 loss: 0.107950814\n",
      "408: accuracy:0.98 loss: 0.080375575\n",
      "409: accuracy:0.98 loss: 0.044408336\n",
      "410: accuracy:0.97 loss: 0.071864545\n",
      "411: accuracy:0.97 loss: 0.088875234\n",
      "412: accuracy:0.98 loss: 0.04029357\n",
      "413: accuracy:0.97 loss: 0.07696074\n",
      "414: accuracy:0.99 loss: 0.023683911\n",
      "415: accuracy:0.98 loss: 0.06857302\n",
      "416: accuracy:0.95 loss: 0.13033846\n",
      "417: accuracy:1.0 loss: 0.010756593\n",
      "418: accuracy:0.97 loss: 0.052616403\n",
      "419: accuracy:0.97 loss: 0.10047351\n",
      "420: accuracy:0.99 loss: 0.034523476\n",
      "421: accuracy:0.98 loss: 0.048522603\n",
      "422: accuracy:0.98 loss: 0.11594735\n",
      "423: accuracy:0.99 loss: 0.05110879\n",
      "424: accuracy:0.98 loss: 0.0506899\n",
      "425: accuracy:0.97 loss: 0.09398024\n",
      "426: accuracy:0.99 loss: 0.05615118\n",
      "427: accuracy:0.98 loss: 0.06323975\n",
      "428: accuracy:0.99 loss: 0.02916675\n",
      "429: accuracy:0.99 loss: 0.066395886\n",
      "430: accuracy:0.98 loss: 0.104591414\n",
      "431: accuracy:0.99 loss: 0.028008873\n",
      "432: accuracy:0.97 loss: 0.06607011\n",
      "433: accuracy:0.96 loss: 0.1120072\n",
      "434: accuracy:0.97 loss: 0.06160859\n",
      "435: accuracy:0.97 loss: 0.079057306\n",
      "436: accuracy:1.0 loss: 0.016028507\n",
      "437: accuracy:0.97 loss: 0.0992687\n",
      "438: accuracy:0.99 loss: 0.044176154\n",
      "439: accuracy:0.96 loss: 0.12965205\n",
      "440: accuracy:0.98 loss: 0.03103694\n",
      "441: accuracy:1.0 loss: 0.019614963\n",
      "442: accuracy:0.98 loss: 0.074098855\n",
      "443: accuracy:0.99 loss: 0.017134726\n",
      "444: accuracy:0.98 loss: 0.15860887\n",
      "445: accuracy:0.98 loss: 0.116542876\n",
      "446: accuracy:0.95 loss: 0.13241304\n",
      "447: accuracy:0.98 loss: 0.10710927\n",
      "448: accuracy:0.96 loss: 0.08226896\n",
      "449: accuracy:0.98 loss: 0.0720829\n",
      "450: accuracy:0.98 loss: 0.1574885\n",
      "451: accuracy:0.96 loss: 0.105152756\n",
      "452: accuracy:0.95 loss: 0.24511355\n",
      "453: accuracy:0.99 loss: 0.044738445\n",
      "454: accuracy:0.97 loss: 0.06643642\n",
      "455: accuracy:0.99 loss: 0.076122805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456: accuracy:0.98 loss: 0.070658915\n",
      "457: accuracy:0.98 loss: 0.04304279\n",
      "458: accuracy:0.96 loss: 0.08991724\n",
      "459: accuracy:0.97 loss: 0.10539034\n",
      "460: accuracy:0.99 loss: 0.11231258\n",
      "461: accuracy:0.98 loss: 0.09130039\n",
      "462: accuracy:0.97 loss: 0.05756754\n",
      "463: accuracy:0.94 loss: 0.11666539\n",
      "464: accuracy:0.98 loss: 0.07792187\n",
      "465: accuracy:0.98 loss: 0.108120434\n",
      "466: accuracy:0.98 loss: 0.056289073\n",
      "467: accuracy:0.97 loss: 0.09331726\n",
      "468: accuracy:0.96 loss: 0.15416048\n",
      "469: accuracy:1.0 loss: 0.014078429\n",
      "470: accuracy:0.98 loss: 0.06416512\n",
      "471: accuracy:1.0 loss: 0.026098799\n",
      "472: accuracy:0.96 loss: 0.08235962\n",
      "473: accuracy:0.96 loss: 0.10129404\n",
      "474: accuracy:1.0 loss: 0.027519118\n",
      "475: accuracy:0.98 loss: 0.060731735\n",
      "476: accuracy:0.97 loss: 0.13697207\n",
      "477: accuracy:0.97 loss: 0.12933873\n",
      "478: accuracy:0.98 loss: 0.04646489\n",
      "479: accuracy:1.0 loss: 0.022313466\n",
      "480: accuracy:0.98 loss: 0.06173381\n",
      "481: accuracy:0.98 loss: 0.035551134\n",
      "482: accuracy:1.0 loss: 0.028654056\n",
      "483: accuracy:0.98 loss: 0.0696893\n",
      "484: accuracy:0.94 loss: 0.16565542\n",
      "485: accuracy:0.97 loss: 0.12867497\n",
      "486: accuracy:0.98 loss: 0.06628823\n",
      "487: accuracy:0.98 loss: 0.036915027\n",
      "488: accuracy:0.97 loss: 0.09208246\n",
      "489: accuracy:0.98 loss: 0.087795794\n",
      "490: accuracy:0.97 loss: 0.096392825\n",
      "491: accuracy:1.0 loss: 0.032301635\n",
      "492: accuracy:0.93 loss: 0.22091687\n",
      "493: accuracy:0.98 loss: 0.037461873\n",
      "494: accuracy:0.98 loss: 0.037782975\n",
      "495: accuracy:0.99 loss: 0.053310867\n",
      "496: accuracy:0.98 loss: 0.07763169\n",
      "497: accuracy:0.96 loss: 0.089365765\n",
      "498: accuracy:0.99 loss: 0.043710824\n",
      "499: accuracy:0.98 loss: 0.078763686\n",
      "500: accuracy:0.98 loss: 0.09364629\n",
      "501: accuracy:0.99 loss: 0.048613213\n",
      "502: accuracy:0.97 loss: 0.10010526\n",
      "503: accuracy:0.98 loss: 0.108957015\n",
      "504: accuracy:1.0 loss: 0.006701816\n",
      "505: accuracy:0.95 loss: 0.11535677\n",
      "506: accuracy:0.99 loss: 0.06588754\n",
      "507: accuracy:0.95 loss: 0.122534886\n",
      "508: accuracy:0.98 loss: 0.047578115\n",
      "509: accuracy:0.99 loss: 0.02664341\n",
      "510: accuracy:0.96 loss: 0.09840718\n",
      "511: accuracy:0.99 loss: 0.05777758\n",
      "512: accuracy:0.98 loss: 0.07181616\n",
      "513: accuracy:0.98 loss: 0.0363964\n",
      "514: accuracy:0.95 loss: 0.18138687\n",
      "515: accuracy:0.98 loss: 0.061538357\n",
      "516: accuracy:0.98 loss: 0.05244062\n",
      "517: accuracy:0.99 loss: 0.032886256\n",
      "518: accuracy:0.99 loss: 0.037027195\n",
      "519: accuracy:0.98 loss: 0.05130434\n",
      "520: accuracy:1.0 loss: 0.028813465\n",
      "521: accuracy:0.97 loss: 0.056567974\n",
      "522: accuracy:0.99 loss: 0.030376926\n",
      "523: accuracy:0.99 loss: 0.06913392\n",
      "524: accuracy:0.99 loss: 0.10252882\n",
      "525: accuracy:0.97 loss: 0.0718497\n",
      "526: accuracy:0.98 loss: 0.09475978\n",
      "527: accuracy:0.98 loss: 0.07531883\n",
      "528: accuracy:0.96 loss: 0.13847625\n",
      "529: accuracy:0.97 loss: 0.09994975\n",
      "530: accuracy:0.98 loss: 0.06862814\n",
      "531: accuracy:0.97 loss: 0.10390243\n",
      "532: accuracy:0.98 loss: 0.20953402\n",
      "533: accuracy:0.99 loss: 0.037751812\n",
      "534: accuracy:0.99 loss: 0.029313035\n",
      "535: accuracy:0.98 loss: 0.0950049\n",
      "536: accuracy:0.98 loss: 0.045994464\n",
      "537: accuracy:0.98 loss: 0.06064064\n",
      "538: accuracy:0.98 loss: 0.036612745\n",
      "539: accuracy:0.99 loss: 0.04349249\n",
      "540: accuracy:0.99 loss: 0.032188818\n",
      "541: accuracy:0.97 loss: 0.085038796\n",
      "542: accuracy:0.96 loss: 0.115720995\n",
      "543: accuracy:0.98 loss: 0.021951726\n",
      "544: accuracy:0.97 loss: 0.17270117\n",
      "545: accuracy:0.99 loss: 0.013683497\n",
      "546: accuracy:0.99 loss: 0.03331634\n",
      "547: accuracy:0.97 loss: 0.08580701\n",
      "548: accuracy:0.99 loss: 0.05854234\n",
      "549: accuracy:0.99 loss: 0.055013925\n",
      "518.2500043064356 107.41648598155007\n",
      "('Train accuracy is  ', 0.9422727351026101)\n",
      "('Train loss is  ', 0.1953027017846365)\n",
      "0: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.072457105\n",
      "1: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.027112976\n",
      "2: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.031500116\n",
      "3: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.032387022\n",
      "4: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.020923026\n",
      "5: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.07592679\n",
      "6: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.02655586\n",
      "7: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.053477596\n",
      "8: ********* epoch 1 ********* test accuracy:0.96 test loss: 0.09948891\n",
      "9: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.058308944\n",
      "10: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.024150677\n",
      "11: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.06034073\n",
      "12: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.031738356\n",
      "13: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.019812942\n",
      "14: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.12092991\n",
      "15: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.042499997\n",
      "16: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.026739774\n",
      "17: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.085119106\n",
      "18: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.05471073\n",
      "19: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.08135543\n",
      "20: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.021436911\n",
      "21: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.037063852\n",
      "22: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.013498454\n",
      "23: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.046456777\n",
      "24: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.022365274\n",
      "25: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.028182968\n",
      "26: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.06477499\n",
      "27: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.06492835\n",
      "28: ********* epoch 1 ********* test accuracy:0.96 test loss: 0.10656526\n",
      "29: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.041622736\n",
      "30: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.038491484\n",
      "31: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.035543036\n",
      "32: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.065181956\n",
      "33: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.045062725\n",
      "34: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.03798914\n",
      "35: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.015142157\n",
      "36: ********* epoch 1 ********* test accuracy:0.95 test loss: 0.07670698\n",
      "37: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.039954137\n",
      "38: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.07055862\n",
      "39: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.030338917\n",
      "40: ********* epoch 1 ********* test accuracy:0.96 test loss: 0.14603418\n",
      "41: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.038809497\n",
      "42: ********* epoch 1 ********* test accuracy:0.95 test loss: 0.08434148\n",
      "43: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.010338345\n",
      "44: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.016565617\n",
      "45: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.05150252\n",
      "46: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.03737719\n",
      "47: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.03847373\n",
      "48: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.04292232\n",
      "49: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.043887302\n",
      "50: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.052713748\n",
      "51: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.014053177\n",
      "52: ********* epoch 1 ********* test accuracy:0.96 test loss: 0.083404616\n",
      "53: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.06324365\n",
      "54: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.031937834\n",
      "55: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.03207977\n",
      "56: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.02633307\n",
      "57: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.1182077\n",
      "58: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.028185353\n",
      "59: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.02821889\n",
      "60: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.06374361\n",
      "61: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.04331687\n",
      "62: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.048821945\n",
      "63: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.020020632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.10203644\n",
      "65: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.03019593\n",
      "66: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.07087984\n",
      "67: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.023167284\n",
      "68: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.10757292\n",
      "69: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.023797905\n",
      "70: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.02035376\n",
      "71: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.033162713\n",
      "72: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.020381512\n",
      "73: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.09308178\n",
      "74: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.015162871\n",
      "75: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.061512228\n",
      "76: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.014596667\n",
      "77: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.12740512\n",
      "78: ********* epoch 1 ********* test accuracy:0.96 test loss: 0.09011488\n",
      "79: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.013029195\n",
      "80: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.018065045\n",
      "81: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.020577913\n",
      "82: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.058680154\n",
      "83: ********* epoch 1 ********* test accuracy:0.96 test loss: 0.09290562\n",
      "84: ********* epoch 1 ********* test accuracy:0.95 test loss: 0.11545103\n",
      "85: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.017042859\n",
      "86: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.11650129\n",
      "87: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.051481448\n",
      "88: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.059542254\n",
      "89: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.037110027\n",
      "90: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.0767569\n",
      "91: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.03521447\n",
      "92: ********* epoch 1 ********* test accuracy:1.0 test loss: 0.009888113\n",
      "93: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.06811061\n",
      "94: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.10968778\n",
      "95: ********* epoch 1 ********* test accuracy:0.95 test loss: 0.14054914\n",
      "96: ********* epoch 1 ********* test accuracy:0.97 test loss: 0.08928835\n",
      "97: ********* epoch 1 ********* test accuracy:0.98 test loss: 0.06652378\n",
      "98: ********* epoch 1 ********* test accuracy:0.99 test loss: 0.055110168\n",
      "99: ********* epoch 1 ********* test accuracy:0.96 test loss: 0.08343731\n",
      "98.2400010228157 5.280309069901705\n",
      "('Test accuracy is  ', 0.982400010228157)\n",
      "('Test loss is  ', 0.05280309069901705)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
