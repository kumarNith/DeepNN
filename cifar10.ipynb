{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kumar/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import data_utils\n",
    "#from cs231n.assignment1.cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#from tensorflow.examples.tutorials.mnist import input_data as mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training data shape: ', (50000, 32, 32, 3))\n",
      "('Training labels shape: ', (50000, 10))\n",
      "('Test data shape: ', (10000, 32, 32, 3))\n",
      "('Test labels shape: ', (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = 'cs231n/assignment1/cs231n/datasets/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = data_utils.load_CIFAR10(cifar10_dir)\n",
    "#y_train = tf.one_hot(y_tn, 10)\n",
    "#y_test = tf.one_hot(y_tst, 10)\n",
    "\n",
    "#print(type(X_train),type(y_train),type(y_test))\n",
    "y_train = enc.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "y_test = enc.fit_transform(y_test.reshape(-1,1)).toarray()\n",
    "\n",
    "#print(type(X_train),type(y_train),type(y_test), type(X_test))\n",
    "\n",
    "#y_train = np.array(y_train)\n",
    "#y_test = np.array(y_test)\n",
    "\n",
    "#print(type(X_train),type(y_train),type(y_test), type(X_test))\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.5.0\n"
     ]
    }
   ],
   "source": [
    "print (\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPlaceHolders(n_width, n_height, n_depth, n_classes):\n",
    "    X = tf.placeholder(tf.float32, [None,n_width, n_height, n_depth ])\n",
    "    y = tf.placeholder(tf.float32,  [None, n_classes])\n",
    "    step = tf.placeholder(tf.int32)\n",
    "    return X,y,step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialize_parameters(K,L,M,N,O,F):\n",
    "    parameters = {}\n",
    "    W1 = tf.get_variable('W1', shape = [3, 3, 3, K], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    B1 = tf.Variable(tf.ones([K])/10)\n",
    "    \n",
    "    W2 = tf.get_variable('W2', shape=[3, 3, K, L], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    B2 = tf.Variable(tf.ones([L])/10)\n",
    "    \n",
    "    W3 = tf.get_variable('W3', shape=[3, 3, L, M], initializer = tf.contrib.layers.xavier_initializer()) \n",
    "    B3 = tf.Variable(tf.ones([M])/10)\n",
    "    \n",
    "    W4 = tf.get_variable('W4', shape=[3, 3, M, N], initializer = tf.contrib.layers.xavier_initializer()) \n",
    "    B4 = tf.Variable(tf.ones([N])/10)\n",
    "    \n",
    "    W5 = tf.get_variable('W5', shape=[3, 3, N, O], initializer = tf.contrib.layers.xavier_initializer()) \n",
    "    B5 = tf.Variable(tf.ones([O])/10)\n",
    "    \n",
    "    W6 = tf.get_variable('W6', shape=[8 * 8 * O, F], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    B6 = tf.Variable(tf.ones([F])/10)\n",
    "    \n",
    "    W7 = tf.get_variable('W7', shape=[F, F], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    B7 = tf.Variable(tf.ones([F])/10)\n",
    "    \n",
    "    W8 = tf.get_variable('W8', shape=[F, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    B8 = tf.Variable(tf.ones([10])/10)\n",
    "    \n",
    "    parameters = {\"W1\":W1, \"B1\":B1, \"W2\":W2, \"B2\":B2,\"W3\":W3,\"B3\":B3,\"W4\":W4, \"B4\":B4,\"W5\":W5, \"B5\":B5, \"W6\":W6, \"B6\":B6,\n",
    "                 \"W7\":W7, \"B7\":B7, \"W8\":W8, \"B8\":B8}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters,X, O, n_inputs, n_classes):\n",
    "    #X,y = createPlaceHolders(n_inputs, n_classes)\n",
    "    #parameters = intialize_parameter(K,L,M,N)\n",
    "    s = 1 # stride\n",
    "    # OUTPUT : 32 * 32 * K\n",
    "    conv1 = tf.nn.relu(tf.nn.conv2d(X, parameters['W1'], strides=[1,s,s,1], padding = 'SAME') + parameters['B1'])\n",
    "    \n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "            padding='SAME', name='pool1') \n",
    "    #max pooling\n",
    "    #pool1 = tf.nn.max_pool()\n",
    "\n",
    "    #s = 2 # stride\n",
    "    # OUTPUT : 16 * 16 * L\n",
    "    conv2 = tf.nn.relu(tf.nn.conv2d(pool1, parameters['W2'], strides=[1,s,s,1], padding = 'SAME') + parameters['B2'])\n",
    "    \n",
    "    #OUTPUT : 8 * 8 * M\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "            padding='SAME', name='pool2')\n",
    "    \n",
    "    #s = 1 # stride\n",
    "    #OUTPUT : 8 * 8 * N\n",
    "    conv3 = tf.nn.relu(tf.nn.conv2d(pool2, parameters['W3'], strides=[1,s,s,1], padding = 'SAME') + parameters['B3'])\n",
    "    '''\n",
    "    New code\n",
    "    '''\n",
    "    #OUTPUT : 8 * 8 * 'O'\n",
    "    conv4 = tf.nn.relu(tf.nn.conv2d(conv3, parameters['W4'], strides=[1,s,s,1], padding = 'SAME') + parameters['B4'])\n",
    "    \n",
    "    #OUTPUT : 8 * 8 * 'O'\n",
    "    conv5 = tf.nn.relu(tf.nn.conv2d(conv4, parameters['W5'], strides=[1,s,s,1], padding = 'SAME') + parameters['B5'])\n",
    "    \n",
    "    #pool3 = tf.nn.max_pool(conv5, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],padding='SAME', name='pool3')\n",
    "    ##################################################\n",
    "    #falttern\n",
    "    yy = tf.reshape(conv5,  [-1, 8 * 8 * O])  # shape =\n",
    "    \n",
    "    fc1 = tf.nn.relu(tf.matmul(yy, parameters['W6']) + parameters['B6'])\n",
    "    \n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, parameters['W7']) + parameters['B7'])\n",
    "    \n",
    "    yLogit = tf.matmul(fc2, parameters['W8']) + parameters['B8']  \n",
    "    \n",
    "    return yLogit\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_loss(yLogit, y):\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits=yLogit, labels=y)\n",
    "    loss = tf.reduce_mean(entropy)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(yLogit, y):\n",
    "    prediction = tf.nn.softmax(yLogit)\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #mnist = mnist_data.read_data_sets(\"data\", one_hot=True)\n",
    "    \n",
    "    n_classes = 10\n",
    "    n_width   = 32\n",
    "    n_height  = 32\n",
    "    n_depth   = 3\n",
    "    n_inputs = n_height * n_width * n_depth # total pixels\n",
    "    \n",
    "    # three convolution layers \n",
    "    K = 96   # First layer depth\n",
    "    L = 128   # Second layer depth\n",
    "    M = 256  # Third layer depth\n",
    "    N = 256\n",
    "    O = 128\n",
    "    F = 1024 # Fully connected \n",
    "\n",
    "    #learning_rate = 0.001\n",
    "    n_epochs = 1\n",
    "    epochs = 1000\n",
    "    batch_size = 64\n",
    "    num_train = X_train.shape[0]\n",
    "    num_test  = X_test.shape[0]\n",
    "    #n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    \n",
    "    X, y, step = createPlaceHolders(n_width, n_height, n_depth, n_classes)\n",
    "    parameters = intialize_parameters(K, L, M, N,O,F)\n",
    "    yLogit     = predict(parameters, X, O, n_inputs, n_classes)\n",
    "    loss       = find_loss(yLogit, y)\n",
    "    accuracy   = find_accuracy(yLogit, y)\n",
    "    \n",
    "    lr = 0.0001 +  tf.train.exponential_decay(0.003, step, 2000, 1/math.e)\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "    \n",
    "    \n",
    "    acc = 0.0\n",
    "    lo = 0.0\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    for i in xrange(epochs):\n",
    "        batch = np.random.choice(num_train, batch_size, replace=True)\n",
    "        #print(X_train.shape, y_train.shape)\n",
    "        batch_X = X_train[batch]\n",
    "        batch_y = y_train[batch]\n",
    "        #print(type(X_train), type(y_train))\n",
    "        #print(type(batch_X), type(batch_y))\n",
    "        #print(X_train.shape, y_train.shape)\n",
    "        #print(batch_X.shape, batch_y.shape)\n",
    "        #batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "        a, c, _,_ = sess.run([accuracy, loss , optimizer,lr], feed_dict={X : batch_X, y : batch_y, step : i})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c))\n",
    "        acc = a + acc\n",
    "        lo = c + lo\n",
    "    print acc,lo\n",
    "    print('Train accuracy is  ', acc/epochs)\n",
    "    print('Train loss is  ', lo/epochs)\n",
    "    acc = 0.0\n",
    "    lo = 0.0\n",
    "    for i in xrange(100):\n",
    "        batch = np.random.choice(num_test, batch_size, replace=True)\n",
    "        batch_X  = X_test[batch]\n",
    "        batch_y  = y_test[batch]\n",
    "        #batch_X, batch_y = mnist.test.next_batch(batch_size)\n",
    "        a, c = sess.run([accuracy, loss], feed_dict={X : batch_X, y : batch_y })\n",
    "        print(str(i) + \": ********* epoch \" + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        acc = a + acc\n",
    "        lo = c+lo\n",
    "    print acc, lo\n",
    "    print('Test accuracy is  ', acc/100)\n",
    "    print('Test loss is  ', lo/100)\n",
    "    sess.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.0625 loss: 10.443335\n",
      "1: accuracy:0.078125 loss: 4843.8057\n",
      "2: accuracy:0.109375 loss: 60.464592\n",
      "3: accuracy:0.125 loss: 13.030361\n",
      "4: accuracy:0.109375 loss: 3.697246\n",
      "5: accuracy:0.109375 loss: 2.892477\n",
      "6: accuracy:0.0625 loss: 5.6485195\n",
      "7: accuracy:0.046875 loss: 3.5829885\n",
      "8: accuracy:0.078125 loss: 2.6503305\n",
      "9: accuracy:0.09375 loss: 3.2024364\n",
      "10: accuracy:0.078125 loss: 2.3408313\n",
      "11: accuracy:0.0625 loss: 2.3654873\n",
      "12: accuracy:0.171875 loss: 2.3076565\n",
      "13: accuracy:0.140625 loss: 2.3056645\n",
      "14: accuracy:0.0625 loss: 2.8077688\n",
      "15: accuracy:0.140625 loss: 2.325612\n",
      "16: accuracy:0.09375 loss: 2.38283\n",
      "17: accuracy:0.125 loss: 2.376186\n",
      "18: accuracy:0.125 loss: 2.3233714\n",
      "19: accuracy:0.046875 loss: 2.3968065\n",
      "20: accuracy:0.109375 loss: 2.4136658\n",
      "21: accuracy:0.046875 loss: 2.3928943\n",
      "22: accuracy:0.09375 loss: 2.3963046\n",
      "23: accuracy:0.0625 loss: 2.3199182\n",
      "24: accuracy:0.09375 loss: 2.3307152\n",
      "25: accuracy:0.125 loss: 2.3758273\n",
      "26: accuracy:0.109375 loss: 2.4058218\n",
      "27: accuracy:0.09375 loss: 2.366155\n",
      "28: accuracy:0.109375 loss: 2.3444295\n",
      "29: accuracy:0.078125 loss: 2.3291216\n",
      "30: accuracy:0.09375 loss: 2.2998228\n",
      "31: accuracy:0.078125 loss: 2.3419456\n",
      "32: accuracy:0.078125 loss: 2.3811717\n",
      "33: accuracy:0.140625 loss: 2.3183417\n",
      "34: accuracy:0.078125 loss: 2.2777033\n",
      "35: accuracy:0.140625 loss: 2.3068194\n",
      "36: accuracy:0.15625 loss: 2.3201687\n",
      "37: accuracy:0.078125 loss: 2.333817\n",
      "38: accuracy:0.171875 loss: 2.2690868\n",
      "39: accuracy:0.078125 loss: 2.2984643\n",
      "40: accuracy:0.15625 loss: 2.2783666\n",
      "41: accuracy:0.078125 loss: 2.3017569\n",
      "42: accuracy:0.09375 loss: 2.2872415\n",
      "43: accuracy:0.0625 loss: 2.2839985\n",
      "44: accuracy:0.171875 loss: 2.303917\n",
      "45: accuracy:0.296875 loss: 2.2217457\n",
      "46: accuracy:0.140625 loss: 2.2616386\n",
      "47: accuracy:0.125 loss: 2.2686343\n",
      "48: accuracy:0.078125 loss: 2.286282\n",
      "49: accuracy:0.21875 loss: 2.208397\n",
      "50: accuracy:0.078125 loss: 2.2794762\n",
      "51: accuracy:0.265625 loss: 2.2225327\n",
      "52: accuracy:0.0625 loss: 2.292396\n",
      "53: accuracy:0.1875 loss: 2.2169914\n",
      "54: accuracy:0.078125 loss: 2.558416\n",
      "55: accuracy:0.109375 loss: 2.2981834\n",
      "56: accuracy:0.15625 loss: 2.191721\n",
      "57: accuracy:0.0625 loss: 2.3420577\n",
      "58: accuracy:0.03125 loss: 2.4002118\n",
      "59: accuracy:0.15625 loss: 2.27511\n",
      "60: accuracy:0.1875 loss: 2.2695007\n",
      "61: accuracy:0.140625 loss: 2.2681174\n",
      "62: accuracy:0.09375 loss: 2.3137207\n",
      "63: accuracy:0.125 loss: 2.2996864\n",
      "64: accuracy:0.140625 loss: 2.2809505\n",
      "65: accuracy:0.109375 loss: 2.2858524\n",
      "66: accuracy:0.0625 loss: 2.3174536\n",
      "67: accuracy:0.140625 loss: 2.225753\n",
      "68: accuracy:0.140625 loss: 2.2025378\n",
      "69: accuracy:0.125 loss: 2.3240693\n",
      "70: accuracy:0.15625 loss: 2.2669494\n",
      "71: accuracy:0.203125 loss: 2.1520963\n",
      "72: accuracy:0.15625 loss: 2.1986828\n",
      "73: accuracy:0.109375 loss: 2.2233515\n",
      "74: accuracy:0.140625 loss: 2.2174053\n",
      "75: accuracy:0.1875 loss: 2.1596708\n",
      "76: accuracy:0.21875 loss: 2.1084838\n",
      "77: accuracy:0.234375 loss: 2.109136\n",
      "78: accuracy:0.234375 loss: 2.1468732\n",
      "79: accuracy:0.140625 loss: 2.1784966\n",
      "80: accuracy:0.171875 loss: 2.173861\n",
      "81: accuracy:0.15625 loss: 2.2775497\n",
      "82: accuracy:0.15625 loss: 2.155541\n",
      "83: accuracy:0.296875 loss: 1.969001\n",
      "84: accuracy:0.171875 loss: 2.112881\n",
      "85: accuracy:0.3125 loss: 2.0934453\n",
      "86: accuracy:0.234375 loss: 2.0867925\n",
      "87: accuracy:0.109375 loss: 2.206947\n",
      "88: accuracy:0.09375 loss: 2.1374402\n",
      "89: accuracy:0.140625 loss: 2.2366056\n",
      "90: accuracy:0.21875 loss: 2.0984938\n",
      "91: accuracy:0.109375 loss: 2.3438394\n",
      "92: accuracy:0.109375 loss: 2.0851126\n",
      "93: accuracy:0.171875 loss: 2.1229715\n",
      "94: accuracy:0.234375 loss: 2.0306654\n",
      "95: accuracy:0.265625 loss: 2.2330651\n",
      "96: accuracy:0.234375 loss: 2.099649\n",
      "97: accuracy:0.171875 loss: 2.2215798\n",
      "98: accuracy:0.171875 loss: 2.1542559\n",
      "99: accuracy:0.09375 loss: 2.2106175\n",
      "100: accuracy:0.171875 loss: 2.1781948\n",
      "101: accuracy:0.203125 loss: 2.0906954\n",
      "102: accuracy:0.21875 loss: 2.0650368\n",
      "103: accuracy:0.140625 loss: 2.1808338\n",
      "104: accuracy:0.1875 loss: 2.1198268\n",
      "105: accuracy:0.265625 loss: 2.0360065\n",
      "106: accuracy:0.203125 loss: 2.1312249\n",
      "107: accuracy:0.0625 loss: 2.3155665\n",
      "108: accuracy:0.21875 loss: 2.0965698\n",
      "109: accuracy:0.21875 loss: 2.126184\n",
      "110: accuracy:0.1875 loss: 2.1683\n",
      "111: accuracy:0.25 loss: 2.0420465\n",
      "112: accuracy:0.21875 loss: 2.1148553\n",
      "113: accuracy:0.21875 loss: 2.1147263\n",
      "114: accuracy:0.21875 loss: 2.038659\n",
      "115: accuracy:0.15625 loss: 2.142311\n",
      "116: accuracy:0.21875 loss: 2.0701296\n",
      "117: accuracy:0.109375 loss: 2.0859118\n",
      "118: accuracy:0.140625 loss: 2.1011329\n",
      "119: accuracy:0.15625 loss: 2.212922\n",
      "120: accuracy:0.234375 loss: 2.1260304\n",
      "121: accuracy:0.125 loss: 2.2255435\n",
      "122: accuracy:0.140625 loss: 2.128264\n",
      "123: accuracy:0.21875 loss: 2.1132061\n",
      "124: accuracy:0.140625 loss: 2.2228198\n",
      "125: accuracy:0.25 loss: 2.055889\n",
      "126: accuracy:0.171875 loss: 1.9875205\n",
      "127: accuracy:0.15625 loss: 2.0451183\n",
      "128: accuracy:0.203125 loss: 2.1469672\n",
      "129: accuracy:0.25 loss: 2.0589566\n",
      "130: accuracy:0.203125 loss: 2.1031528\n",
      "131: accuracy:0.171875 loss: 2.0780878\n",
      "132: accuracy:0.171875 loss: 2.1540027\n",
      "133: accuracy:0.203125 loss: 2.0600314\n",
      "134: accuracy:0.203125 loss: 2.1265962\n",
      "135: accuracy:0.140625 loss: 2.2746575\n",
      "136: accuracy:0.21875 loss: 2.0890918\n",
      "137: accuracy:0.203125 loss: 2.017261\n",
      "138: accuracy:0.25 loss: 2.1263623\n",
      "139: accuracy:0.15625 loss: 2.3022494\n",
      "140: accuracy:0.1875 loss: 2.066852\n",
      "141: accuracy:0.234375 loss: 2.1344292\n",
      "142: accuracy:0.203125 loss: 2.2936678\n",
      "143: accuracy:0.171875 loss: 2.191622\n",
      "144: accuracy:0.265625 loss: 2.0062623\n",
      "145: accuracy:0.1875 loss: 2.2057326\n",
      "146: accuracy:0.171875 loss: 2.0408592\n",
      "147: accuracy:0.1875 loss: 2.1867027\n",
      "148: accuracy:0.1875 loss: 2.083053\n",
      "149: accuracy:0.1875 loss: 2.1374938\n",
      "150: accuracy:0.203125 loss: 2.1121745\n",
      "151: accuracy:0.1875 loss: 2.2345347\n",
      "152: accuracy:0.171875 loss: 2.1109595\n",
      "153: accuracy:0.34375 loss: 1.9380753\n",
      "154: accuracy:0.1875 loss: 2.0784154\n",
      "155: accuracy:0.1875 loss: 2.1432772\n",
      "156: accuracy:0.140625 loss: 2.2725482\n",
      "157: accuracy:0.265625 loss: 1.9993516\n",
      "158: accuracy:0.265625 loss: 2.0241914\n",
      "159: accuracy:0.203125 loss: 2.1338577\n",
      "160: accuracy:0.171875 loss: 2.0852284\n",
      "161: accuracy:0.109375 loss: 2.0652046\n",
      "162: accuracy:0.140625 loss: 2.0812173\n",
      "163: accuracy:0.140625 loss: 2.1148071\n",
      "164: accuracy:0.21875 loss: 2.0423727\n",
      "165: accuracy:0.1875 loss: 2.0364804\n",
      "166: accuracy:0.28125 loss: 1.9652207\n",
      "167: accuracy:0.125 loss: 2.1088846\n",
      "168: accuracy:0.203125 loss: 2.036214\n",
      "169: accuracy:0.28125 loss: 2.1211736\n",
      "170: accuracy:0.234375 loss: 2.1104403\n",
      "171: accuracy:0.28125 loss: 2.1690392\n",
      "172: accuracy:0.265625 loss: 2.0381935\n",
      "173: accuracy:0.328125 loss: 1.9157252\n",
      "174: accuracy:0.15625 loss: 2.1171527\n",
      "175: accuracy:0.234375 loss: 2.103363\n",
      "176: accuracy:0.25 loss: 2.0302944\n",
      "177: accuracy:0.1875 loss: 2.0442896\n",
      "178: accuracy:0.296875 loss: 1.9725391\n",
      "179: accuracy:0.203125 loss: 2.0044184\n",
      "180: accuracy:0.296875 loss: 1.8658748\n",
      "181: accuracy:0.296875 loss: 1.898875\n",
      "182: accuracy:0.25 loss: 1.9803718\n",
      "183: accuracy:0.171875 loss: 2.0807319\n",
      "184: accuracy:0.171875 loss: 2.0301137\n",
      "185: accuracy:0.203125 loss: 1.8555721\n",
      "186: accuracy:0.375 loss: 1.7391722\n",
      "187: accuracy:0.28125 loss: 1.922374\n",
      "188: accuracy:0.203125 loss: 1.9912891\n",
      "189: accuracy:0.328125 loss: 1.9248052\n",
      "190: accuracy:0.234375 loss: 2.0507617\n",
      "191: accuracy:0.3125 loss: 2.0064003\n",
      "192: accuracy:0.296875 loss: 1.8985523\n",
      "193: accuracy:0.25 loss: 1.9519769\n",
      "194: accuracy:0.28125 loss: 2.0405245\n",
      "195: accuracy:0.375 loss: 1.9641368\n",
      "196: accuracy:0.171875 loss: 2.1587462\n",
      "197: accuracy:0.234375 loss: 1.9970174\n",
      "198: accuracy:0.203125 loss: 2.0534782\n",
      "199: accuracy:0.171875 loss: 2.066484\n",
      "200: accuracy:0.3125 loss: 1.9376895\n",
      "201: accuracy:0.234375 loss: 1.9970526\n",
      "202: accuracy:0.1875 loss: 2.0177803\n",
      "203: accuracy:0.203125 loss: 2.0319667\n",
      "204: accuracy:0.25 loss: 2.199567\n",
      "205: accuracy:0.265625 loss: 1.8752902\n",
      "206: accuracy:0.25 loss: 1.8884282\n",
      "207: accuracy:0.25 loss: 1.9379814\n",
      "208: accuracy:0.265625 loss: 2.0077024\n",
      "209: accuracy:0.21875 loss: 1.9767919\n",
      "210: accuracy:0.3125 loss: 1.8448207\n",
      "211: accuracy:0.265625 loss: 2.0222664\n",
      "212: accuracy:0.25 loss: 1.8970163\n",
      "213: accuracy:0.265625 loss: 2.0267866\n",
      "214: accuracy:0.359375 loss: 1.8083404\n",
      "215: accuracy:0.21875 loss: 1.9484737\n",
      "216: accuracy:0.296875 loss: 1.8817422\n",
      "217: accuracy:0.15625 loss: 2.1302354\n",
      "218: accuracy:0.265625 loss: 2.0498395\n",
      "219: accuracy:0.296875 loss: 1.9552099\n",
      "220: accuracy:0.234375 loss: 1.9956517\n",
      "221: accuracy:0.328125 loss: 2.0430238\n",
      "222: accuracy:0.25 loss: 2.0132236\n",
      "223: accuracy:0.28125 loss: 1.9600317\n",
      "224: accuracy:0.296875 loss: 1.9262624\n",
      "225: accuracy:0.21875 loss: 1.9764397\n",
      "226: accuracy:0.1875 loss: 2.1421685\n",
      "227: accuracy:0.234375 loss: 1.8935549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228: accuracy:0.359375 loss: 1.7908658\n",
      "229: accuracy:0.28125 loss: 1.766748\n",
      "230: accuracy:0.234375 loss: 2.0070002\n",
      "231: accuracy:0.21875 loss: 1.9274843\n",
      "232: accuracy:0.296875 loss: 2.012517\n",
      "233: accuracy:0.3125 loss: 1.930645\n",
      "234: accuracy:0.34375 loss: 1.8897874\n",
      "235: accuracy:0.25 loss: 2.027147\n",
      "236: accuracy:0.234375 loss: 1.9919751\n",
      "237: accuracy:0.234375 loss: 2.0307717\n",
      "238: accuracy:0.265625 loss: 1.9074578\n",
      "239: accuracy:0.21875 loss: 1.9405515\n",
      "240: accuracy:0.28125 loss: 1.8684604\n",
      "241: accuracy:0.296875 loss: 2.051521\n",
      "242: accuracy:0.125 loss: 2.5471213\n",
      "243: accuracy:0.171875 loss: 2.10929\n",
      "244: accuracy:0.3125 loss: 1.8296273\n",
      "245: accuracy:0.234375 loss: 2.1197743\n",
      "246: accuracy:0.25 loss: 1.9151626\n",
      "247: accuracy:0.125 loss: 2.1968746\n",
      "248: accuracy:0.296875 loss: 2.034807\n",
      "249: accuracy:0.203125 loss: 2.1861005\n",
      "250: accuracy:0.1875 loss: 2.1917815\n",
      "251: accuracy:0.21875 loss: 1.9979202\n",
      "252: accuracy:0.265625 loss: 1.9527557\n",
      "253: accuracy:0.28125 loss: 1.999589\n",
      "254: accuracy:0.296875 loss: 2.022067\n",
      "255: accuracy:0.328125 loss: 1.8436596\n",
      "256: accuracy:0.265625 loss: 1.8188841\n",
      "257: accuracy:0.234375 loss: 1.9626377\n",
      "258: accuracy:0.1875 loss: 2.0927777\n",
      "259: accuracy:0.3125 loss: 1.9449496\n",
      "260: accuracy:0.21875 loss: 2.1324272\n",
      "261: accuracy:0.265625 loss: 1.9296854\n",
      "262: accuracy:0.234375 loss: 2.0369093\n",
      "263: accuracy:0.25 loss: 2.0961213\n",
      "264: accuracy:0.359375 loss: 2.0005596\n",
      "265: accuracy:0.25 loss: 1.996797\n",
      "266: accuracy:0.375 loss: 1.803323\n",
      "267: accuracy:0.28125 loss: 1.8038402\n",
      "268: accuracy:0.21875 loss: 2.1126847\n",
      "269: accuracy:0.296875 loss: 1.9056213\n",
      "270: accuracy:0.265625 loss: 2.0128734\n",
      "271: accuracy:0.296875 loss: 1.9064941\n",
      "272: accuracy:0.34375 loss: 1.9182525\n",
      "273: accuracy:0.40625 loss: 1.7241862\n",
      "274: accuracy:0.21875 loss: 1.9105718\n",
      "275: accuracy:0.296875 loss: 1.8542297\n",
      "276: accuracy:0.28125 loss: 1.8284237\n",
      "277: accuracy:0.265625 loss: 1.9406631\n",
      "278: accuracy:0.296875 loss: 2.1026187\n",
      "279: accuracy:0.28125 loss: 1.9469817\n",
      "280: accuracy:0.40625 loss: 1.8664021\n",
      "281: accuracy:0.25 loss: 2.0086384\n",
      "282: accuracy:0.28125 loss: 1.9839755\n",
      "283: accuracy:0.21875 loss: 1.9619844\n",
      "284: accuracy:0.375 loss: 1.9128777\n",
      "285: accuracy:0.234375 loss: 2.0927024\n",
      "286: accuracy:0.234375 loss: 1.901813\n",
      "287: accuracy:0.359375 loss: 1.8899283\n",
      "288: accuracy:0.21875 loss: 2.0426645\n",
      "289: accuracy:0.265625 loss: 1.9255817\n",
      "290: accuracy:0.1875 loss: 2.002698\n",
      "291: accuracy:0.25 loss: 1.9847498\n",
      "292: accuracy:0.265625 loss: 1.9657272\n",
      "293: accuracy:0.1875 loss: 2.2301111\n",
      "294: accuracy:0.28125 loss: 1.999741\n",
      "295: accuracy:0.234375 loss: 2.0376182\n",
      "296: accuracy:0.21875 loss: 1.9762444\n",
      "297: accuracy:0.1875 loss: 1.9788522\n",
      "298: accuracy:0.28125 loss: 2.0057523\n",
      "299: accuracy:0.296875 loss: 1.8193314\n",
      "300: accuracy:0.25 loss: 1.9368131\n",
      "301: accuracy:0.40625 loss: 1.9297774\n",
      "302: accuracy:0.296875 loss: 1.8669903\n",
      "303: accuracy:0.28125 loss: 1.9619793\n",
      "304: accuracy:0.328125 loss: 1.8390552\n",
      "305: accuracy:0.265625 loss: 2.1057482\n",
      "306: accuracy:0.34375 loss: 1.8078023\n",
      "307: accuracy:0.34375 loss: 1.9396839\n",
      "308: accuracy:0.34375 loss: 1.9148991\n",
      "309: accuracy:0.34375 loss: 1.9905816\n",
      "310: accuracy:0.34375 loss: 1.8349348\n",
      "311: accuracy:0.234375 loss: 1.9922881\n",
      "312: accuracy:0.265625 loss: 1.9165787\n",
      "313: accuracy:0.265625 loss: 1.8980906\n",
      "314: accuracy:0.328125 loss: 1.8470018\n",
      "315: accuracy:0.40625 loss: 1.7265\n",
      "316: accuracy:0.3125 loss: 1.9143393\n",
      "317: accuracy:0.28125 loss: 1.9008162\n",
      "318: accuracy:0.421875 loss: 1.7806352\n",
      "319: accuracy:0.296875 loss: 1.9318688\n",
      "320: accuracy:0.40625 loss: 1.7574677\n",
      "321: accuracy:0.421875 loss: 1.6218033\n",
      "322: accuracy:0.34375 loss: 1.7911048\n",
      "323: accuracy:0.28125 loss: 1.8508736\n",
      "324: accuracy:0.28125 loss: 1.9634044\n",
      "325: accuracy:0.390625 loss: 1.6942085\n",
      "326: accuracy:0.3125 loss: 1.9686462\n",
      "327: accuracy:0.265625 loss: 2.0267954\n",
      "328: accuracy:0.25 loss: 1.8964956\n",
      "329: accuracy:0.3125 loss: 1.8968718\n",
      "330: accuracy:0.34375 loss: 1.7147114\n",
      "331: accuracy:0.234375 loss: 1.9349929\n",
      "332: accuracy:0.390625 loss: 1.779192\n",
      "333: accuracy:0.265625 loss: 1.9242319\n",
      "334: accuracy:0.28125 loss: 1.9637977\n",
      "335: accuracy:0.4375 loss: 1.7213976\n",
      "336: accuracy:0.359375 loss: 1.7803501\n",
      "337: accuracy:0.359375 loss: 1.8843838\n",
      "338: accuracy:0.375 loss: 1.8093798\n",
      "339: accuracy:0.234375 loss: 1.8918831\n",
      "340: accuracy:0.265625 loss: 1.8103619\n",
      "341: accuracy:0.234375 loss: 1.9494387\n",
      "342: accuracy:0.359375 loss: 1.8619848\n",
      "343: accuracy:0.234375 loss: 2.0298858\n",
      "344: accuracy:0.25 loss: 2.0776649\n",
      "345: accuracy:0.40625 loss: 1.8452858\n",
      "346: accuracy:0.46875 loss: 1.7329835\n",
      "347: accuracy:0.25 loss: 1.9164677\n",
      "348: accuracy:0.234375 loss: 1.9075377\n",
      "349: accuracy:0.171875 loss: 2.311338\n",
      "350: accuracy:0.28125 loss: 1.8485\n",
      "351: accuracy:0.25 loss: 2.1174393\n",
      "352: accuracy:0.203125 loss: 1.9535887\n",
      "353: accuracy:0.34375 loss: 1.866735\n",
      "354: accuracy:0.375 loss: 1.8599937\n",
      "355: accuracy:0.328125 loss: 1.8871135\n",
      "356: accuracy:0.265625 loss: 1.9632299\n",
      "357: accuracy:0.234375 loss: 2.083212\n",
      "358: accuracy:0.234375 loss: 1.9118208\n",
      "359: accuracy:0.15625 loss: 2.0992436\n",
      "360: accuracy:0.265625 loss: 1.9709594\n",
      "361: accuracy:0.328125 loss: 1.8965161\n",
      "362: accuracy:0.296875 loss: 1.9309998\n",
      "363: accuracy:0.21875 loss: 2.030538\n",
      "364: accuracy:0.21875 loss: 1.8382612\n",
      "365: accuracy:0.234375 loss: 1.8624195\n",
      "366: accuracy:0.328125 loss: 1.80565\n",
      "367: accuracy:0.34375 loss: 1.8890464\n",
      "368: accuracy:0.390625 loss: 1.9089463\n",
      "369: accuracy:0.28125 loss: 2.0725899\n",
      "370: accuracy:0.359375 loss: 1.849222\n",
      "371: accuracy:0.3125 loss: 1.9036896\n",
      "372: accuracy:0.296875 loss: 1.9066747\n",
      "373: accuracy:0.265625 loss: 1.9415429\n",
      "374: accuracy:0.328125 loss: 1.8356953\n",
      "375: accuracy:0.359375 loss: 1.6999483\n",
      "376: accuracy:0.21875 loss: 1.8803973\n",
      "377: accuracy:0.28125 loss: 2.060288\n",
      "378: accuracy:0.453125 loss: 1.744776\n",
      "379: accuracy:0.359375 loss: 1.7471104\n",
      "380: accuracy:0.203125 loss: 1.8332393\n",
      "381: accuracy:0.375 loss: 1.9234533\n",
      "382: accuracy:0.34375 loss: 1.9616504\n",
      "383: accuracy:0.21875 loss: 1.9927284\n",
      "384: accuracy:0.34375 loss: 1.7793514\n",
      "385: accuracy:0.34375 loss: 1.9834238\n",
      "386: accuracy:0.40625 loss: 1.750423\n",
      "387: accuracy:0.359375 loss: 1.7470262\n",
      "388: accuracy:0.265625 loss: 1.877993\n",
      "389: accuracy:0.296875 loss: 1.8653826\n",
      "390: accuracy:0.421875 loss: 1.6026244\n",
      "391: accuracy:0.375 loss: 1.8210901\n",
      "392: accuracy:0.3125 loss: 1.9143254\n",
      "393: accuracy:0.3125 loss: 1.8767521\n",
      "394: accuracy:0.390625 loss: 1.6928697\n",
      "395: accuracy:0.296875 loss: 1.862469\n",
      "396: accuracy:0.453125 loss: 1.5918818\n",
      "397: accuracy:0.296875 loss: 1.9353753\n",
      "398: accuracy:0.359375 loss: 1.7551489\n",
      "399: accuracy:0.390625 loss: 1.7231256\n",
      "400: accuracy:0.34375 loss: 1.7769434\n",
      "401: accuracy:0.34375 loss: 1.9855855\n",
      "402: accuracy:0.3125 loss: 1.7805786\n",
      "403: accuracy:0.40625 loss: 1.7878842\n",
      "404: accuracy:0.296875 loss: 1.730661\n",
      "405: accuracy:0.359375 loss: 1.7184461\n",
      "406: accuracy:0.3125 loss: 1.8837204\n",
      "407: accuracy:0.375 loss: 1.6808465\n",
      "408: accuracy:0.296875 loss: 1.8419087\n",
      "409: accuracy:0.25 loss: 1.9676641\n",
      "410: accuracy:0.3125 loss: 1.839534\n",
      "411: accuracy:0.328125 loss: 2.0083275\n",
      "412: accuracy:0.4375 loss: 1.7035214\n",
      "413: accuracy:0.265625 loss: 2.0532858\n",
      "414: accuracy:0.21875 loss: 2.0108907\n",
      "415: accuracy:0.390625 loss: 1.6751989\n",
      "416: accuracy:0.34375 loss: 1.8353515\n",
      "417: accuracy:0.265625 loss: 1.901935\n",
      "418: accuracy:0.34375 loss: 1.852182\n",
      "419: accuracy:0.390625 loss: 1.8280566\n",
      "420: accuracy:0.34375 loss: 1.7666209\n",
      "421: accuracy:0.40625 loss: 1.7227743\n",
      "422: accuracy:0.3125 loss: 1.8769808\n",
      "423: accuracy:0.328125 loss: 1.8654604\n",
      "424: accuracy:0.28125 loss: 1.9486735\n",
      "425: accuracy:0.3125 loss: 1.9340264\n",
      "426: accuracy:0.34375 loss: 1.9247253\n",
      "427: accuracy:0.328125 loss: 1.8349413\n",
      "428: accuracy:0.25 loss: 1.8135221\n",
      "429: accuracy:0.375 loss: 1.76272\n",
      "430: accuracy:0.34375 loss: 1.7144074\n",
      "431: accuracy:0.328125 loss: 1.8983777\n",
      "432: accuracy:0.40625 loss: 1.8579906\n",
      "433: accuracy:0.375 loss: 1.7808315\n",
      "434: accuracy:0.296875 loss: 1.748744\n",
      "435: accuracy:0.46875 loss: 1.8312564\n",
      "436: accuracy:0.390625 loss: 1.719177\n",
      "437: accuracy:0.421875 loss: 1.649981\n",
      "438: accuracy:0.453125 loss: 1.72468\n",
      "439: accuracy:0.28125 loss: 1.7825186\n",
      "440: accuracy:0.28125 loss: 1.8911724\n",
      "441: accuracy:0.484375 loss: 1.6462125\n",
      "442: accuracy:0.25 loss: 1.9795325\n",
      "443: accuracy:0.203125 loss: 2.0741787\n",
      "444: accuracy:0.171875 loss: 2.1048293\n",
      "445: accuracy:0.296875 loss: 1.8479823\n",
      "446: accuracy:0.390625 loss: 1.8206515\n",
      "447: accuracy:0.390625 loss: 1.8440256\n",
      "448: accuracy:0.265625 loss: 1.8098719\n",
      "449: accuracy:0.3125 loss: 1.8510091\n",
      "450: accuracy:0.265625 loss: 1.8255808\n",
      "451: accuracy:0.28125 loss: 2.0583804\n",
      "452: accuracy:0.296875 loss: 1.9819283\n",
      "453: accuracy:0.3125 loss: 1.8493838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454: accuracy:0.390625 loss: 1.8271841\n",
      "455: accuracy:0.25 loss: 1.9276515\n",
      "456: accuracy:0.328125 loss: 1.888476\n",
      "457: accuracy:0.234375 loss: 1.9487395\n",
      "458: accuracy:0.3125 loss: 1.9440781\n",
      "459: accuracy:0.5 loss: 1.7370532\n",
      "460: accuracy:0.46875 loss: 1.5935103\n",
      "461: accuracy:0.265625 loss: 1.7679147\n",
      "462: accuracy:0.296875 loss: 1.9537944\n",
      "463: accuracy:0.390625 loss: 1.7566227\n",
      "464: accuracy:0.34375 loss: 1.8029798\n",
      "465: accuracy:0.3125 loss: 1.7827708\n",
      "466: accuracy:0.390625 loss: 1.649928\n",
      "467: accuracy:0.21875 loss: 1.9543958\n",
      "468: accuracy:0.375 loss: 1.8112974\n",
      "469: accuracy:0.25 loss: 1.8429978\n",
      "470: accuracy:0.328125 loss: 1.67429\n",
      "471: accuracy:0.28125 loss: 1.7464154\n",
      "472: accuracy:0.3125 loss: 1.9335431\n",
      "473: accuracy:0.34375 loss: 1.6635543\n",
      "474: accuracy:0.203125 loss: 1.9875417\n",
      "475: accuracy:0.21875 loss: 1.9377445\n",
      "476: accuracy:0.296875 loss: 1.8384873\n",
      "477: accuracy:0.40625 loss: 1.8343444\n",
      "478: accuracy:0.375 loss: 1.858696\n",
      "479: accuracy:0.3125 loss: 1.8604407\n",
      "480: accuracy:0.390625 loss: 1.6820599\n",
      "481: accuracy:0.203125 loss: 2.0887241\n",
      "482: accuracy:0.390625 loss: 1.8079145\n",
      "483: accuracy:0.40625 loss: 1.8009064\n",
      "484: accuracy:0.4375 loss: 1.6119035\n",
      "485: accuracy:0.359375 loss: 1.8044614\n",
      "486: accuracy:0.359375 loss: 1.795058\n",
      "487: accuracy:0.3125 loss: 1.8288698\n",
      "488: accuracy:0.328125 loss: 1.7052531\n",
      "489: accuracy:0.296875 loss: 1.676758\n",
      "490: accuracy:0.328125 loss: 1.8470259\n",
      "491: accuracy:0.375 loss: 1.8588754\n",
      "492: accuracy:0.359375 loss: 1.7269675\n",
      "493: accuracy:0.375 loss: 1.7056172\n",
      "494: accuracy:0.328125 loss: 1.8811579\n",
      "495: accuracy:0.375 loss: 1.6904482\n",
      "496: accuracy:0.34375 loss: 1.8165555\n",
      "497: accuracy:0.359375 loss: 1.8628393\n",
      "498: accuracy:0.328125 loss: 1.7953784\n",
      "499: accuracy:0.34375 loss: 1.7155344\n",
      "500: accuracy:0.3125 loss: 1.8024464\n",
      "501: accuracy:0.34375 loss: 1.8268659\n",
      "502: accuracy:0.375 loss: 1.7919967\n",
      "503: accuracy:0.375 loss: 1.7441121\n",
      "504: accuracy:0.375 loss: 1.7262633\n",
      "505: accuracy:0.328125 loss: 1.5821573\n",
      "506: accuracy:0.203125 loss: 2.0528572\n",
      "507: accuracy:0.296875 loss: 1.8246307\n",
      "508: accuracy:0.359375 loss: 1.7656636\n",
      "509: accuracy:0.296875 loss: 1.8015599\n",
      "510: accuracy:0.375 loss: 1.8210181\n",
      "511: accuracy:0.328125 loss: 1.8022757\n",
      "512: accuracy:0.34375 loss: 1.6992841\n",
      "513: accuracy:0.375 loss: 1.7896633\n",
      "514: accuracy:0.375 loss: 1.5592403\n",
      "515: accuracy:0.34375 loss: 1.8720838\n",
      "516: accuracy:0.390625 loss: 1.6957414\n",
      "517: accuracy:0.296875 loss: 1.781498\n",
      "518: accuracy:0.328125 loss: 1.775923\n",
      "519: accuracy:0.296875 loss: 1.8290155\n",
      "520: accuracy:0.34375 loss: 1.7215412\n",
      "521: accuracy:0.421875 loss: 1.7633697\n",
      "522: accuracy:0.25 loss: 1.9325361\n",
      "523: accuracy:0.3125 loss: 1.7570937\n",
      "524: accuracy:0.46875 loss: 1.5503347\n",
      "525: accuracy:0.421875 loss: 1.7961178\n",
      "526: accuracy:0.28125 loss: 1.7066625\n",
      "527: accuracy:0.40625 loss: 1.5713303\n",
      "528: accuracy:0.375 loss: 1.6338043\n",
      "529: accuracy:0.21875 loss: 2.0411263\n",
      "530: accuracy:0.453125 loss: 1.599751\n",
      "531: accuracy:0.359375 loss: 1.7612338\n",
      "532: accuracy:0.265625 loss: 1.8313468\n",
      "533: accuracy:0.359375 loss: 1.7218293\n",
      "534: accuracy:0.3125 loss: 1.8719723\n",
      "535: accuracy:0.390625 loss: 1.6767111\n",
      "536: accuracy:0.234375 loss: 2.0681295\n",
      "537: accuracy:0.359375 loss: 1.8532631\n",
      "538: accuracy:0.328125 loss: 1.6847154\n",
      "539: accuracy:0.28125 loss: 1.9384955\n",
      "540: accuracy:0.328125 loss: 1.6995709\n",
      "541: accuracy:0.296875 loss: 1.8218656\n",
      "542: accuracy:0.34375 loss: 1.8398623\n",
      "543: accuracy:0.359375 loss: 1.779562\n",
      "544: accuracy:0.390625 loss: 1.6027708\n",
      "545: accuracy:0.296875 loss: 1.8515718\n",
      "546: accuracy:0.46875 loss: 1.6976248\n",
      "547: accuracy:0.3125 loss: 1.862257\n",
      "548: accuracy:0.171875 loss: 1.9378914\n",
      "549: accuracy:0.359375 loss: 1.851653\n",
      "550: accuracy:0.296875 loss: 1.8564818\n",
      "551: accuracy:0.375 loss: 1.841594\n",
      "552: accuracy:0.34375 loss: 1.7455666\n",
      "553: accuracy:0.4375 loss: 1.5973842\n",
      "554: accuracy:0.328125 loss: 1.8189335\n",
      "555: accuracy:0.390625 loss: 1.8491279\n",
      "556: accuracy:0.40625 loss: 1.5899357\n",
      "557: accuracy:0.375 loss: 1.7484626\n",
      "558: accuracy:0.4375 loss: 1.6560826\n",
      "559: accuracy:0.375 loss: 1.6755657\n",
      "560: accuracy:0.296875 loss: 2.0329864\n",
      "561: accuracy:0.21875 loss: 2.1085892\n",
      "562: accuracy:0.28125 loss: 1.8829726\n",
      "563: accuracy:0.375 loss: 1.5139222\n",
      "564: accuracy:0.34375 loss: 1.681795\n",
      "565: accuracy:0.296875 loss: 1.8112588\n",
      "566: accuracy:0.265625 loss: 1.8332016\n",
      "567: accuracy:0.375 loss: 1.8295428\n",
      "568: accuracy:0.3125 loss: 1.9620433\n",
      "569: accuracy:0.390625 loss: 1.7308178\n",
      "570: accuracy:0.421875 loss: 1.6476262\n",
      "571: accuracy:0.375 loss: 1.870209\n",
      "572: accuracy:0.203125 loss: 2.1933396\n",
      "573: accuracy:0.140625 loss: 2.0678277\n",
      "574: accuracy:0.390625 loss: 1.7864599\n",
      "575: accuracy:0.4375 loss: 1.7560194\n",
      "576: accuracy:0.359375 loss: 1.8298986\n",
      "577: accuracy:0.328125 loss: 1.8557597\n",
      "578: accuracy:0.359375 loss: 1.9005709\n",
      "579: accuracy:0.375 loss: 1.8169236\n",
      "580: accuracy:0.28125 loss: 1.9113458\n",
      "581: accuracy:0.25 loss: 2.0598257\n",
      "582: accuracy:0.359375 loss: 1.7886903\n",
      "583: accuracy:0.21875 loss: 1.8930228\n",
      "584: accuracy:0.3125 loss: 1.678807\n",
      "585: accuracy:0.375 loss: 1.8478736\n",
      "586: accuracy:0.265625 loss: 1.966674\n",
      "587: accuracy:0.265625 loss: 1.7842064\n",
      "588: accuracy:0.375 loss: 1.7762634\n",
      "589: accuracy:0.3125 loss: 1.8168411\n",
      "590: accuracy:0.34375 loss: 1.784893\n",
      "591: accuracy:0.390625 loss: 1.6289229\n",
      "592: accuracy:0.265625 loss: 1.9545145\n",
      "593: accuracy:0.328125 loss: 1.789176\n",
      "594: accuracy:0.375 loss: 1.8387685\n",
      "595: accuracy:0.28125 loss: 1.9582264\n",
      "596: accuracy:0.421875 loss: 1.6490166\n",
      "597: accuracy:0.265625 loss: 1.939504\n",
      "598: accuracy:0.359375 loss: 1.736962\n",
      "599: accuracy:0.3125 loss: 1.7927334\n",
      "600: accuracy:0.421875 loss: 1.6994972\n",
      "601: accuracy:0.375 loss: 1.774245\n",
      "602: accuracy:0.40625 loss: 1.8518277\n",
      "603: accuracy:0.3125 loss: 1.8762416\n",
      "604: accuracy:0.296875 loss: 1.8502111\n",
      "605: accuracy:0.265625 loss: 1.9274921\n",
      "606: accuracy:0.28125 loss: 1.7928238\n",
      "607: accuracy:0.328125 loss: 1.9133891\n",
      "608: accuracy:0.359375 loss: 1.7830426\n",
      "609: accuracy:0.34375 loss: 1.748662\n",
      "610: accuracy:0.234375 loss: 1.9731336\n",
      "611: accuracy:0.328125 loss: 1.6448846\n",
      "612: accuracy:0.328125 loss: 1.7990286\n",
      "613: accuracy:0.375 loss: 1.7643387\n",
      "614: accuracy:0.40625 loss: 1.7818022\n",
      "615: accuracy:0.375 loss: 1.7303188\n",
      "616: accuracy:0.28125 loss: 1.8187478\n",
      "617: accuracy:0.40625 loss: 1.7408055\n",
      "618: accuracy:0.3125 loss: 1.8187158\n",
      "619: accuracy:0.34375 loss: 1.7678142\n",
      "620: accuracy:0.4375 loss: 1.7238849\n",
      "621: accuracy:0.265625 loss: 1.8652153\n",
      "622: accuracy:0.375 loss: 1.6906729\n",
      "623: accuracy:0.296875 loss: 1.8155969\n",
      "624: accuracy:0.328125 loss: 1.7412574\n",
      "625: accuracy:0.296875 loss: 1.9229454\n",
      "626: accuracy:0.375 loss: 1.8395216\n",
      "627: accuracy:0.40625 loss: 1.5538573\n",
      "628: accuracy:0.46875 loss: 1.7338167\n",
      "629: accuracy:0.34375 loss: 1.7701845\n",
      "630: accuracy:0.328125 loss: 1.7682127\n",
      "631: accuracy:0.46875 loss: 1.6640126\n",
      "632: accuracy:0.296875 loss: 1.8424344\n",
      "633: accuracy:0.25 loss: 1.9338723\n",
      "634: accuracy:0.3125 loss: 1.7419916\n",
      "635: accuracy:0.265625 loss: 1.794991\n",
      "636: accuracy:0.3125 loss: 1.7972432\n",
      "637: accuracy:0.421875 loss: 1.7952454\n",
      "638: accuracy:0.34375 loss: 1.7109778\n",
      "639: accuracy:0.375 loss: 1.7487689\n",
      "640: accuracy:0.359375 loss: 1.762017\n",
      "641: accuracy:0.34375 loss: 1.7388643\n",
      "642: accuracy:0.328125 loss: 1.9268138\n",
      "643: accuracy:0.375 loss: 1.7139605\n",
      "644: accuracy:0.28125 loss: 1.8386962\n",
      "645: accuracy:0.328125 loss: 1.9427608\n",
      "646: accuracy:0.234375 loss: 1.995784\n",
      "647: accuracy:0.453125 loss: 1.4913094\n",
      "648: accuracy:0.40625 loss: 1.5985091\n",
      "649: accuracy:0.390625 loss: 1.7296128\n",
      "650: accuracy:0.328125 loss: 1.7212843\n",
      "651: accuracy:0.359375 loss: 1.8697821\n",
      "652: accuracy:0.421875 loss: 1.5865283\n",
      "653: accuracy:0.34375 loss: 1.8111684\n",
      "654: accuracy:0.359375 loss: 1.7428864\n",
      "655: accuracy:0.375 loss: 1.8939854\n",
      "656: accuracy:0.3125 loss: 1.703412\n",
      "657: accuracy:0.46875 loss: 1.5905355\n",
      "658: accuracy:0.375 loss: 1.695837\n",
      "659: accuracy:0.4375 loss: 1.5638187\n",
      "660: accuracy:0.40625 loss: 1.7427028\n",
      "661: accuracy:0.296875 loss: 1.6688256\n",
      "662: accuracy:0.3125 loss: 1.7280624\n",
      "663: accuracy:0.359375 loss: 1.7419851\n",
      "664: accuracy:0.28125 loss: 1.8155395\n",
      "665: accuracy:0.4375 loss: 1.5146701\n",
      "666: accuracy:0.328125 loss: 1.845911\n",
      "667: accuracy:0.265625 loss: 1.8673674\n",
      "668: accuracy:0.296875 loss: 1.7417121\n",
      "669: accuracy:0.296875 loss: 1.8313173\n",
      "670: accuracy:0.265625 loss: 1.8152344\n",
      "671: accuracy:0.421875 loss: 1.7019601\n",
      "672: accuracy:0.234375 loss: 1.8433318\n",
      "673: accuracy:0.28125 loss: 1.991069\n",
      "674: accuracy:0.28125 loss: 1.8701158\n",
      "675: accuracy:0.390625 loss: 1.6672481\n",
      "676: accuracy:0.453125 loss: 1.7479713\n",
      "677: accuracy:0.375 loss: 1.8942059\n",
      "678: accuracy:0.375 loss: 1.7271688\n",
      "679: accuracy:0.453125 loss: 1.6360271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680: accuracy:0.40625 loss: 1.657563\n",
      "681: accuracy:0.359375 loss: 1.74786\n",
      "682: accuracy:0.265625 loss: 1.9149857\n",
      "683: accuracy:0.390625 loss: 1.6498299\n",
      "684: accuracy:0.28125 loss: 1.9867097\n",
      "685: accuracy:0.390625 loss: 1.757831\n",
      "686: accuracy:0.390625 loss: 1.6567796\n",
      "687: accuracy:0.4375 loss: 1.6347069\n",
      "688: accuracy:0.34375 loss: 1.7012842\n",
      "689: accuracy:0.28125 loss: 1.7435988\n",
      "690: accuracy:0.25 loss: 1.9820523\n",
      "691: accuracy:0.3125 loss: 1.9028103\n",
      "692: accuracy:0.359375 loss: 1.60663\n",
      "693: accuracy:0.359375 loss: 1.6171573\n",
      "694: accuracy:0.328125 loss: 1.7168024\n",
      "695: accuracy:0.359375 loss: 1.8741368\n",
      "696: accuracy:0.34375 loss: 1.8071932\n",
      "697: accuracy:0.328125 loss: 1.6917275\n",
      "698: accuracy:0.375 loss: 1.8209127\n",
      "699: accuracy:0.375 loss: 1.8354983\n",
      "700: accuracy:0.34375 loss: 1.8428261\n",
      "701: accuracy:0.34375 loss: 1.621765\n",
      "702: accuracy:0.359375 loss: 1.7834127\n",
      "703: accuracy:0.375 loss: 1.8682716\n",
      "704: accuracy:0.328125 loss: 1.685554\n",
      "705: accuracy:0.265625 loss: 1.8457611\n",
      "706: accuracy:0.390625 loss: 1.7852786\n",
      "707: accuracy:0.375 loss: 1.7558185\n",
      "708: accuracy:0.484375 loss: 1.5231233\n",
      "709: accuracy:0.375 loss: 1.7167411\n",
      "710: accuracy:0.390625 loss: 1.6672999\n",
      "711: accuracy:0.375 loss: 1.8569059\n",
      "712: accuracy:0.375 loss: 1.6638741\n",
      "713: accuracy:0.328125 loss: 1.8144659\n",
      "714: accuracy:0.453125 loss: 1.515053\n",
      "715: accuracy:0.328125 loss: 1.8923404\n",
      "716: accuracy:0.40625 loss: 1.6261232\n",
      "717: accuracy:0.234375 loss: 1.8323094\n",
      "718: accuracy:0.484375 loss: 1.4863607\n",
      "719: accuracy:0.25 loss: 1.999649\n",
      "720: accuracy:0.375 loss: 1.8940861\n",
      "721: accuracy:0.46875 loss: 1.6328988\n",
      "722: accuracy:0.296875 loss: 1.7466235\n",
      "723: accuracy:0.40625 loss: 1.6983945\n",
      "724: accuracy:0.40625 loss: 1.6142033\n",
      "725: accuracy:0.34375 loss: 1.744257\n",
      "726: accuracy:0.4375 loss: 1.5600995\n",
      "727: accuracy:0.4375 loss: 1.8221315\n",
      "728: accuracy:0.4375 loss: 1.6875498\n",
      "729: accuracy:0.25 loss: 1.9531181\n",
      "730: accuracy:0.3125 loss: 1.7764046\n",
      "731: accuracy:0.28125 loss: 1.920828\n",
      "732: accuracy:0.265625 loss: 1.9702692\n",
      "733: accuracy:0.4375 loss: 1.6112733\n",
      "734: accuracy:0.328125 loss: 1.720449\n",
      "735: accuracy:0.421875 loss: 1.6556851\n",
      "736: accuracy:0.390625 loss: 1.6791108\n",
      "737: accuracy:0.375 loss: 1.7897592\n",
      "738: accuracy:0.359375 loss: 1.7442638\n",
      "739: accuracy:0.28125 loss: 1.8581502\n",
      "740: accuracy:0.453125 loss: 1.5690612\n",
      "741: accuracy:0.296875 loss: 1.8106004\n",
      "742: accuracy:0.453125 loss: 1.6594219\n",
      "743: accuracy:0.40625 loss: 1.4877248\n",
      "744: accuracy:0.390625 loss: 1.6596363\n",
      "745: accuracy:0.46875 loss: 1.5540221\n",
      "746: accuracy:0.34375 loss: 1.7689128\n",
      "747: accuracy:0.359375 loss: 1.8236566\n",
      "748: accuracy:0.421875 loss: 1.5789579\n",
      "749: accuracy:0.390625 loss: 1.7779665\n",
      "750: accuracy:0.375 loss: 1.6528571\n",
      "751: accuracy:0.421875 loss: 1.8271997\n",
      "752: accuracy:0.484375 loss: 1.7666824\n",
      "753: accuracy:0.375 loss: 1.8373301\n",
      "754: accuracy:0.4375 loss: 1.7145851\n",
      "755: accuracy:0.375 loss: 1.794783\n",
      "756: accuracy:0.390625 loss: 1.6970303\n",
      "757: accuracy:0.359375 loss: 1.6493273\n",
      "758: accuracy:0.4375 loss: 1.630389\n",
      "759: accuracy:0.40625 loss: 1.8478762\n",
      "760: accuracy:0.40625 loss: 1.5277289\n",
      "761: accuracy:0.421875 loss: 1.7770705\n",
      "762: accuracy:0.328125 loss: 1.8037251\n",
      "763: accuracy:0.359375 loss: 1.7111104\n",
      "764: accuracy:0.4375 loss: 1.6625867\n",
      "765: accuracy:0.390625 loss: 1.7360098\n",
      "766: accuracy:0.4375 loss: 1.6932554\n",
      "767: accuracy:0.28125 loss: 1.9867485\n",
      "768: accuracy:0.375 loss: 1.6688154\n",
      "769: accuracy:0.375 loss: 1.8451867\n",
      "770: accuracy:0.328125 loss: 2.0470297\n",
      "771: accuracy:0.359375 loss: 1.7768953\n",
      "772: accuracy:0.3125 loss: 1.8428677\n",
      "773: accuracy:0.375 loss: 1.6710906\n",
      "774: accuracy:0.296875 loss: 1.7678044\n",
      "775: accuracy:0.421875 loss: 1.6268466\n",
      "776: accuracy:0.375 loss: 1.7264705\n",
      "777: accuracy:0.421875 loss: 1.5508088\n",
      "778: accuracy:0.265625 loss: 2.0170434\n",
      "779: accuracy:0.40625 loss: 1.5443611\n",
      "780: accuracy:0.375 loss: 1.760719\n",
      "781: accuracy:0.25 loss: 2.0235245\n",
      "782: accuracy:0.3125 loss: 1.7930429\n",
      "783: accuracy:0.390625 loss: 1.7608079\n",
      "784: accuracy:0.34375 loss: 1.7714834\n",
      "785: accuracy:0.453125 loss: 1.576285\n",
      "786: accuracy:0.453125 loss: 1.5902637\n",
      "787: accuracy:0.4375 loss: 1.6194559\n",
      "788: accuracy:0.421875 loss: 1.5944791\n",
      "789: accuracy:0.390625 loss: 1.8289287\n",
      "790: accuracy:0.34375 loss: 1.7210205\n",
      "791: accuracy:0.4375 loss: 1.5359191\n",
      "792: accuracy:0.375 loss: 1.6461687\n",
      "793: accuracy:0.375 loss: 1.7340766\n",
      "794: accuracy:0.40625 loss: 1.6580799\n",
      "795: accuracy:0.375 loss: 1.9004376\n",
      "796: accuracy:0.40625 loss: 1.6274614\n",
      "797: accuracy:0.40625 loss: 1.6085657\n",
      "798: accuracy:0.421875 loss: 1.4869953\n",
      "799: accuracy:0.34375 loss: 1.8178093\n",
      "800: accuracy:0.390625 loss: 1.7130218\n",
      "801: accuracy:0.34375 loss: 1.9429463\n",
      "802: accuracy:0.359375 loss: 1.6308035\n",
      "803: accuracy:0.46875 loss: 1.5999339\n",
      "804: accuracy:0.328125 loss: 1.8355175\n",
      "805: accuracy:0.328125 loss: 1.9434576\n",
      "806: accuracy:0.390625 loss: 1.7917082\n",
      "807: accuracy:0.3125 loss: 1.7680725\n",
      "808: accuracy:0.390625 loss: 1.6875567\n",
      "809: accuracy:0.375 loss: 1.6969078\n",
      "810: accuracy:0.421875 loss: 1.6557748\n",
      "811: accuracy:0.328125 loss: 1.7167294\n",
      "812: accuracy:0.375 loss: 1.7888222\n",
      "813: accuracy:0.375 loss: 1.6906984\n",
      "814: accuracy:0.453125 loss: 1.6804099\n",
      "815: accuracy:0.34375 loss: 1.7318091\n",
      "816: accuracy:0.375 loss: 1.6574206\n",
      "817: accuracy:0.4375 loss: 1.5978749\n",
      "818: accuracy:0.3125 loss: 1.8305895\n",
      "819: accuracy:0.296875 loss: 1.9024655\n",
      "820: accuracy:0.421875 loss: 1.5733666\n",
      "821: accuracy:0.328125 loss: 1.7670611\n",
      "822: accuracy:0.3125 loss: 1.7257129\n",
      "823: accuracy:0.328125 loss: 1.7097783\n",
      "824: accuracy:0.3125 loss: 1.6451137\n",
      "825: accuracy:0.453125 loss: 1.5132096\n",
      "826: accuracy:0.40625 loss: 1.6050904\n",
      "827: accuracy:0.3125 loss: 1.9262704\n",
      "828: accuracy:0.46875 loss: 1.6155696\n",
      "829: accuracy:0.34375 loss: 1.7896066\n",
      "830: accuracy:0.296875 loss: 1.8953717\n",
      "831: accuracy:0.296875 loss: 1.8042692\n",
      "832: accuracy:0.3125 loss: 1.6684377\n",
      "833: accuracy:0.4375 loss: 1.5905398\n",
      "834: accuracy:0.375 loss: 1.7155125\n",
      "835: accuracy:0.359375 loss: 1.8312972\n",
      "836: accuracy:0.375 loss: 1.5802026\n",
      "837: accuracy:0.375 loss: 1.6862497\n",
      "838: accuracy:0.5 loss: 1.5002692\n",
      "839: accuracy:0.359375 loss: 1.9146576\n",
      "840: accuracy:0.421875 loss: 1.7424152\n",
      "841: accuracy:0.390625 loss: 1.6154488\n",
      "842: accuracy:0.3125 loss: 1.6550369\n",
      "843: accuracy:0.421875 loss: 1.5396266\n",
      "844: accuracy:0.25 loss: 1.8156774\n",
      "845: accuracy:0.359375 loss: 1.6029642\n",
      "846: accuracy:0.28125 loss: 1.7031983\n",
      "847: accuracy:0.25 loss: 1.8920171\n",
      "848: accuracy:0.421875 loss: 1.7734201\n",
      "849: accuracy:0.46875 loss: 1.6226174\n",
      "850: accuracy:0.390625 loss: 1.5594394\n",
      "851: accuracy:0.40625 loss: 1.6843224\n",
      "852: accuracy:0.484375 loss: 1.659573\n",
      "853: accuracy:0.40625 loss: 1.7493978\n",
      "854: accuracy:0.40625 loss: 1.6474053\n",
      "855: accuracy:0.40625 loss: 1.7334479\n",
      "856: accuracy:0.3125 loss: 1.7396826\n",
      "857: accuracy:0.34375 loss: 1.8257221\n",
      "858: accuracy:0.296875 loss: 1.8668352\n",
      "859: accuracy:0.375 loss: 1.5601081\n",
      "860: accuracy:0.375 loss: 1.6980801\n",
      "861: accuracy:0.328125 loss: 1.7355374\n",
      "862: accuracy:0.34375 loss: 1.751299\n",
      "863: accuracy:0.421875 loss: 1.625742\n",
      "864: accuracy:0.34375 loss: 1.6738288\n",
      "865: accuracy:0.375 loss: 1.6349397\n",
      "866: accuracy:0.40625 loss: 1.7506905\n",
      "867: accuracy:0.453125 loss: 1.584528\n",
      "868: accuracy:0.40625 loss: 1.5638901\n",
      "869: accuracy:0.3125 loss: 1.7913773\n",
      "870: accuracy:0.421875 loss: 1.5835403\n",
      "871: accuracy:0.359375 loss: 1.7636411\n",
      "872: accuracy:0.5 loss: 1.4761\n",
      "873: accuracy:0.359375 loss: 1.8158231\n",
      "874: accuracy:0.4375 loss: 1.5586221\n",
      "875: accuracy:0.40625 loss: 1.7324688\n",
      "876: accuracy:0.34375 loss: 1.7441912\n",
      "877: accuracy:0.359375 loss: 1.6876543\n",
      "878: accuracy:0.484375 loss: 1.6102998\n",
      "879: accuracy:0.421875 loss: 1.5170443\n",
      "880: accuracy:0.390625 loss: 1.7647753\n",
      "881: accuracy:0.421875 loss: 1.6793784\n",
      "882: accuracy:0.421875 loss: 1.7034519\n",
      "883: accuracy:0.46875 loss: 1.5321816\n",
      "884: accuracy:0.390625 loss: 1.6783454\n",
      "885: accuracy:0.328125 loss: 1.9186614\n",
      "886: accuracy:0.390625 loss: 1.7275112\n",
      "887: accuracy:0.4375 loss: 1.7064426\n",
      "888: accuracy:0.375 loss: 1.7900598\n",
      "889: accuracy:0.46875 loss: 1.4774626\n",
      "890: accuracy:0.3125 loss: 1.8867767\n",
      "891: accuracy:0.390625 loss: 1.7986827\n",
      "892: accuracy:0.46875 loss: 1.725524\n",
      "893: accuracy:0.4375 loss: 1.4565731\n",
      "894: accuracy:0.4375 loss: 1.7306508\n",
      "895: accuracy:0.390625 loss: 1.6203074\n",
      "896: accuracy:0.28125 loss: 1.8176303\n",
      "897: accuracy:0.421875 loss: 1.7254993\n",
      "898: accuracy:0.359375 loss: 1.6714475\n",
      "899: accuracy:0.359375 loss: 1.7302363\n",
      "900: accuracy:0.34375 loss: 1.5841656\n",
      "901: accuracy:0.34375 loss: 1.8083637\n",
      "902: accuracy:0.375 loss: 1.5904462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903: accuracy:0.359375 loss: 1.5226858\n",
      "904: accuracy:0.375 loss: 1.6463282\n",
      "905: accuracy:0.453125 loss: 1.6098554\n",
      "906: accuracy:0.390625 loss: 1.6371559\n",
      "907: accuracy:0.453125 loss: 1.5895295\n",
      "908: accuracy:0.359375 loss: 1.604965\n",
      "909: accuracy:0.375 loss: 1.750766\n",
      "910: accuracy:0.28125 loss: 1.8867939\n",
      "911: accuracy:0.453125 loss: 1.5203328\n",
      "912: accuracy:0.421875 loss: 1.764841\n",
      "913: accuracy:0.359375 loss: 1.752958\n",
      "914: accuracy:0.40625 loss: 1.769166\n",
      "915: accuracy:0.5 loss: 1.4842038\n",
      "916: accuracy:0.40625 loss: 1.5413188\n",
      "917: accuracy:0.3125 loss: 1.8477292\n",
      "918: accuracy:0.34375 loss: 1.8549414\n",
      "919: accuracy:0.296875 loss: 1.7413505\n",
      "920: accuracy:0.3125 loss: 1.9294037\n",
      "921: accuracy:0.390625 loss: 1.6617683\n",
      "922: accuracy:0.390625 loss: 1.5911286\n",
      "923: accuracy:0.359375 loss: 1.5994691\n",
      "924: accuracy:0.34375 loss: 1.7358816\n",
      "925: accuracy:0.328125 loss: 1.8151941\n",
      "926: accuracy:0.4375 loss: 1.6920637\n",
      "927: accuracy:0.390625 loss: 1.6090798\n",
      "928: accuracy:0.40625 loss: 1.7108344\n",
      "929: accuracy:0.328125 loss: 1.8442976\n",
      "930: accuracy:0.421875 loss: 1.7211473\n",
      "931: accuracy:0.328125 loss: 1.6960539\n",
      "932: accuracy:0.328125 loss: 1.7874014\n",
      "933: accuracy:0.234375 loss: 2.0005844\n",
      "934: accuracy:0.375 loss: 1.6706476\n",
      "935: accuracy:0.375 loss: 1.7639258\n",
      "936: accuracy:0.40625 loss: 1.688214\n",
      "937: accuracy:0.390625 loss: 1.6598785\n",
      "938: accuracy:0.484375 loss: 1.6022606\n",
      "939: accuracy:0.453125 loss: 1.5316861\n",
      "940: accuracy:0.421875 loss: 1.7594476\n",
      "941: accuracy:0.375 loss: 1.7471511\n",
      "942: accuracy:0.484375 loss: 1.5054247\n",
      "943: accuracy:0.421875 loss: 1.5936477\n",
      "944: accuracy:0.390625 loss: 1.6646192\n",
      "945: accuracy:0.375 loss: 2.0099144\n",
      "946: accuracy:0.328125 loss: 1.8608474\n",
      "947: accuracy:0.375 loss: 1.634638\n",
      "948: accuracy:0.4375 loss: 1.5478368\n",
      "949: accuracy:0.359375 loss: 1.7223487\n",
      "950: accuracy:0.46875 loss: 1.58604\n",
      "951: accuracy:0.3125 loss: 1.7830672\n",
      "952: accuracy:0.40625 loss: 1.7205367\n",
      "953: accuracy:0.28125 loss: 1.7901258\n",
      "954: accuracy:0.359375 loss: 1.709532\n",
      "955: accuracy:0.421875 loss: 1.5996144\n",
      "956: accuracy:0.40625 loss: 1.6075809\n",
      "957: accuracy:0.421875 loss: 1.6731842\n",
      "958: accuracy:0.265625 loss: 1.9523823\n",
      "959: accuracy:0.5 loss: 1.482847\n",
      "960: accuracy:0.328125 loss: 1.9527423\n",
      "961: accuracy:0.390625 loss: 1.6642773\n",
      "962: accuracy:0.375 loss: 1.7050307\n",
      "963: accuracy:0.328125 loss: 1.7101943\n",
      "964: accuracy:0.390625 loss: 1.6829326\n",
      "965: accuracy:0.40625 loss: 1.6873324\n",
      "966: accuracy:0.34375 loss: 1.7053702\n",
      "967: accuracy:0.28125 loss: 1.7665908\n",
      "968: accuracy:0.390625 loss: 1.5926086\n",
      "969: accuracy:0.453125 loss: 1.5438461\n",
      "970: accuracy:0.4375 loss: 1.5509088\n",
      "971: accuracy:0.421875 loss: 1.6912639\n",
      "972: accuracy:0.390625 loss: 1.5862384\n",
      "973: accuracy:0.484375 loss: 1.5105119\n",
      "974: accuracy:0.390625 loss: 1.6444049\n",
      "975: accuracy:0.375 loss: 1.6337546\n",
      "976: accuracy:0.46875 loss: 1.4797862\n",
      "977: accuracy:0.265625 loss: 1.8657753\n",
      "978: accuracy:0.328125 loss: 1.7261207\n",
      "979: accuracy:0.3125 loss: 1.7727544\n",
      "980: accuracy:0.390625 loss: 1.7877537\n",
      "981: accuracy:0.421875 loss: 1.6246815\n",
      "982: accuracy:0.328125 loss: 1.8542069\n",
      "983: accuracy:0.28125 loss: 1.7367995\n",
      "984: accuracy:0.359375 loss: 1.6328325\n",
      "985: accuracy:0.390625 loss: 1.8352897\n",
      "986: accuracy:0.484375 loss: 1.5412458\n",
      "987: accuracy:0.34375 loss: 1.7509711\n",
      "988: accuracy:0.390625 loss: 1.53357\n",
      "989: accuracy:0.34375 loss: 1.760699\n",
      "990: accuracy:0.375 loss: 1.601001\n",
      "991: accuracy:0.40625 loss: 1.5874645\n",
      "992: accuracy:0.3125 loss: 1.7231736\n",
      "993: accuracy:0.5 loss: 1.5348269\n",
      "994: accuracy:0.4375 loss: 1.4449061\n",
      "995: accuracy:0.375 loss: 1.6274265\n",
      "996: accuracy:0.453125 loss: 1.6502165\n",
      "997: accuracy:0.453125 loss: 1.5639116\n",
      "998: accuracy:0.484375 loss: 1.475174\n",
      "999: accuracy:0.265625 loss: 1.8533636\n",
      "307.5 6799.247648358345\n",
      "('Train accuracy is  ', 0.3075)\n",
      "('Train loss is  ', 6.799247648358345)\n",
      "0: ********* epoch  ********* test accuracy:0.3125 test loss: 1.7622681\n",
      "1: ********* epoch  ********* test accuracy:0.390625 test loss: 1.7296895\n",
      "2: ********* epoch  ********* test accuracy:0.453125 test loss: 1.7030171\n",
      "3: ********* epoch  ********* test accuracy:0.328125 test loss: 1.6755266\n",
      "4: ********* epoch  ********* test accuracy:0.4375 test loss: 1.5499692\n",
      "5: ********* epoch  ********* test accuracy:0.375 test loss: 1.7199346\n",
      "6: ********* epoch  ********* test accuracy:0.34375 test loss: 1.7194755\n",
      "7: ********* epoch  ********* test accuracy:0.359375 test loss: 1.8182011\n",
      "8: ********* epoch  ********* test accuracy:0.359375 test loss: 1.7535827\n",
      "9: ********* epoch  ********* test accuracy:0.5 test loss: 1.6771516\n",
      "10: ********* epoch  ********* test accuracy:0.390625 test loss: 1.7293715\n",
      "11: ********* epoch  ********* test accuracy:0.453125 test loss: 1.5468597\n",
      "12: ********* epoch  ********* test accuracy:0.4375 test loss: 1.4956927\n",
      "13: ********* epoch  ********* test accuracy:0.5 test loss: 1.550767\n",
      "14: ********* epoch  ********* test accuracy:0.40625 test loss: 1.6975963\n",
      "15: ********* epoch  ********* test accuracy:0.40625 test loss: 1.6252356\n",
      "16: ********* epoch  ********* test accuracy:0.46875 test loss: 1.5064293\n",
      "17: ********* epoch  ********* test accuracy:0.390625 test loss: 1.5200151\n",
      "18: ********* epoch  ********* test accuracy:0.4375 test loss: 1.5381598\n",
      "19: ********* epoch  ********* test accuracy:0.484375 test loss: 1.5732844\n",
      "20: ********* epoch  ********* test accuracy:0.375 test loss: 1.6560268\n",
      "21: ********* epoch  ********* test accuracy:0.40625 test loss: 1.7610142\n",
      "22: ********* epoch  ********* test accuracy:0.390625 test loss: 1.6889805\n",
      "23: ********* epoch  ********* test accuracy:0.34375 test loss: 1.70523\n",
      "24: ********* epoch  ********* test accuracy:0.3125 test loss: 1.6716849\n",
      "25: ********* epoch  ********* test accuracy:0.4375 test loss: 1.6187458\n",
      "26: ********* epoch  ********* test accuracy:0.34375 test loss: 1.7361916\n",
      "27: ********* epoch  ********* test accuracy:0.46875 test loss: 1.5880235\n",
      "28: ********* epoch  ********* test accuracy:0.375 test loss: 1.7431705\n",
      "29: ********* epoch  ********* test accuracy:0.453125 test loss: 1.5650654\n",
      "30: ********* epoch  ********* test accuracy:0.375 test loss: 1.8155458\n",
      "31: ********* epoch  ********* test accuracy:0.453125 test loss: 1.4250069\n",
      "32: ********* epoch  ********* test accuracy:0.34375 test loss: 1.8488225\n",
      "33: ********* epoch  ********* test accuracy:0.40625 test loss: 1.6375864\n",
      "34: ********* epoch  ********* test accuracy:0.390625 test loss: 1.7189162\n",
      "35: ********* epoch  ********* test accuracy:0.375 test loss: 1.4531937\n",
      "36: ********* epoch  ********* test accuracy:0.21875 test loss: 1.7778438\n",
      "37: ********* epoch  ********* test accuracy:0.421875 test loss: 1.6386222\n",
      "38: ********* epoch  ********* test accuracy:0.484375 test loss: 1.5598407\n",
      "39: ********* epoch  ********* test accuracy:0.328125 test loss: 1.7091495\n",
      "40: ********* epoch  ********* test accuracy:0.375 test loss: 1.6566237\n",
      "41: ********* epoch  ********* test accuracy:0.40625 test loss: 1.6636007\n",
      "42: ********* epoch  ********* test accuracy:0.484375 test loss: 1.3975313\n",
      "43: ********* epoch  ********* test accuracy:0.40625 test loss: 1.6064842\n",
      "44: ********* epoch  ********* test accuracy:0.4375 test loss: 1.6157482\n",
      "45: ********* epoch  ********* test accuracy:0.453125 test loss: 1.5809574\n",
      "46: ********* epoch  ********* test accuracy:0.28125 test loss: 1.6934165\n",
      "47: ********* epoch  ********* test accuracy:0.328125 test loss: 1.9167745\n",
      "48: ********* epoch  ********* test accuracy:0.453125 test loss: 1.4810892\n",
      "49: ********* epoch  ********* test accuracy:0.484375 test loss: 1.4879129\n",
      "50: ********* epoch  ********* test accuracy:0.328125 test loss: 1.8357658\n",
      "51: ********* epoch  ********* test accuracy:0.453125 test loss: 1.5552769\n",
      "52: ********* epoch  ********* test accuracy:0.359375 test loss: 1.8338479\n",
      "53: ********* epoch  ********* test accuracy:0.515625 test loss: 1.6549172\n",
      "54: ********* epoch  ********* test accuracy:0.390625 test loss: 1.5914369\n",
      "55: ********* epoch  ********* test accuracy:0.359375 test loss: 1.8346037\n",
      "56: ********* epoch  ********* test accuracy:0.328125 test loss: 1.9552797\n",
      "57: ********* epoch  ********* test accuracy:0.484375 test loss: 1.451451\n",
      "58: ********* epoch  ********* test accuracy:0.375 test loss: 1.6766019\n",
      "59: ********* epoch  ********* test accuracy:0.296875 test loss: 1.8464879\n",
      "60: ********* epoch  ********* test accuracy:0.453125 test loss: 1.5704687\n",
      "61: ********* epoch  ********* test accuracy:0.40625 test loss: 1.6181538\n",
      "62: ********* epoch  ********* test accuracy:0.265625 test loss: 1.7919195\n",
      "63: ********* epoch  ********* test accuracy:0.375 test loss: 1.6967392\n",
      "64: ********* epoch  ********* test accuracy:0.421875 test loss: 1.6811306\n",
      "65: ********* epoch  ********* test accuracy:0.28125 test loss: 1.9173808\n",
      "66: ********* epoch  ********* test accuracy:0.40625 test loss: 1.7076073\n",
      "67: ********* epoch  ********* test accuracy:0.4375 test loss: 1.5795525\n",
      "68: ********* epoch  ********* test accuracy:0.390625 test loss: 1.8569479\n",
      "69: ********* epoch  ********* test accuracy:0.390625 test loss: 1.6501707\n",
      "70: ********* epoch  ********* test accuracy:0.390625 test loss: 1.6661336\n",
      "71: ********* epoch  ********* test accuracy:0.46875 test loss: 1.4998407\n",
      "72: ********* epoch  ********* test accuracy:0.265625 test loss: 1.9426562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73: ********* epoch  ********* test accuracy:0.390625 test loss: 1.6487644\n",
      "74: ********* epoch  ********* test accuracy:0.34375 test loss: 1.6424075\n",
      "75: ********* epoch  ********* test accuracy:0.40625 test loss: 1.6970112\n",
      "76: ********* epoch  ********* test accuracy:0.390625 test loss: 1.7874639\n",
      "77: ********* epoch  ********* test accuracy:0.375 test loss: 1.7044489\n",
      "78: ********* epoch  ********* test accuracy:0.3125 test loss: 1.7884096\n",
      "79: ********* epoch  ********* test accuracy:0.390625 test loss: 1.7023814\n",
      "80: ********* epoch  ********* test accuracy:0.34375 test loss: 1.5755265\n",
      "81: ********* epoch  ********* test accuracy:0.1875 test loss: 1.8890799\n",
      "82: ********* epoch  ********* test accuracy:0.28125 test loss: 1.939431\n",
      "83: ********* epoch  ********* test accuracy:0.421875 test loss: 1.5867829\n",
      "84: ********* epoch  ********* test accuracy:0.296875 test loss: 1.8451083\n",
      "85: ********* epoch  ********* test accuracy:0.359375 test loss: 1.8658171\n",
      "86: ********* epoch  ********* test accuracy:0.375 test loss: 1.6768963\n",
      "87: ********* epoch  ********* test accuracy:0.453125 test loss: 1.5000553\n",
      "88: ********* epoch  ********* test accuracy:0.421875 test loss: 1.581059\n",
      "89: ********* epoch  ********* test accuracy:0.34375 test loss: 1.625617\n",
      "90: ********* epoch  ********* test accuracy:0.40625 test loss: 1.4888933\n",
      "91: ********* epoch  ********* test accuracy:0.375 test loss: 1.5328755\n",
      "92: ********* epoch  ********* test accuracy:0.359375 test loss: 1.7170918\n",
      "93: ********* epoch  ********* test accuracy:0.328125 test loss: 1.6323274\n",
      "94: ********* epoch  ********* test accuracy:0.5 test loss: 1.5502639\n",
      "95: ********* epoch  ********* test accuracy:0.4375 test loss: 1.6872101\n",
      "96: ********* epoch  ********* test accuracy:0.359375 test loss: 1.5491209\n",
      "97: ********* epoch  ********* test accuracy:0.359375 test loss: 1.6695071\n",
      "98: ********* epoch  ********* test accuracy:0.453125 test loss: 1.593433\n",
      "99: ********* epoch  ********* test accuracy:0.4375 test loss: 1.710383\n",
      "39.0 166.91276335716248\n",
      "('Test accuracy is  ', 0.39)\n",
      "('Test loss is  ', 1.6691276335716247)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
